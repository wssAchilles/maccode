% !TEX root = ../main.tex
\chapter{数据预处理与探索性分析}

\section{数据清洗与质量分析}

原始能耗数据需经过系统化的 ETL（Extract-Transform-Load）流程，才能转化为机器学习模型可用的训练数据。

\subsection{数据清洗与聚合}

原始数据存在多楼层分散存储、列名不统一、时间粒度过细等问题，需通过清洗和聚合操作进行标准化。

\subsubsection{功率列自动识别与聚合}
每个楼层的 CSV 文件包含数十个功率测点（如 \texttt{HVAC\_kW}、\texttt{Lighting\_kW} 等）。需聚合为单一的楼层总负载：
\begin{equation}
    L_{\text{floor}}(t) = \sum_{i=1}^{N} P_i(t)
\end{equation}

\subsubsection{多楼层合并}
不同楼层的数据文件需按时间戳对齐合并。系统采用内连接（Inner Join）策略，确保仅保留所有楼层均有记录的时间点。合并后计算全站总负载 $L_{\text{site}}(t) = \sum_{f=1}^{F} L_{\text{floor},f}(t)$。

\subsubsection{无效数据过滤}
原始数据中存在日期解析异常的记录。系统在读取阶段将无法解析的日期转换为 \texttt{NaT}，随后删除这些无效行，确保时间索引的完整性。

\subsection{数据清洗与聚合}

数据清洗过程由 \texttt{data\_processor.py} 模块中的 \texttt{EnergyDataProcessor} 类实现。针对原始分钟级数据，我们采用了以下清洗策略：

\begin{enumerate}
    \item \textbf{时间对齐与异常处理}：使用 Pandas 解析 \texttt{Date} 列。针对原始数据中可能存在的解析错误，系统在读取阶段通过 \texttt{errors='coerce'} 参数将无效日期强制转换为 \texttt{NaT}，并随后剔除这些行，确保时间索引的单调性与合法性。
    \item \textbf{智能功率聚合}：由于原始 CSV 文件中包含数十个分项功率测点（如 HVAC, Lighting 等），且命名规则不统一。系统实现了基于关键字匹配的聚合策略：通过扫描列名中是否包含 \texttt{"kW"} 关键字，自动识别所有功率列并求和计算 \texttt{Total\_Load}，从而规避了硬编码列名的维护风险。
    \item \textbf{多楼层数据融合}：针对多层建筑的数据分散问题，系统采用\textbf{内连接 (Inner Join)} 策略合并不同楼层的数据。这意味着仅保留所有楼层在同一时刻均有有效记录的时间点，从而保证了全屋负载计算 ($L_{\text{site}} = \sum L_{\text{floor}}$) 的时间一致性，避免了因单层数据缺失导致的整体负载低估。
    \item \textbf{降采样与缺失值填充}：原始数据为分钟级采样，存在高频噪声且与电价计费周期（小时级）不匹配。我们使用 \texttt{resample('1H').mean()} 将数据按小时重采样，取该时段内的平均功率。对于重采样过程中产生的瞬时数据缺失（例如某小时内少于60个点），系统采用\textbf{前向填充 (Forward Fill)} 策略，假设缺失时段的系统状态与上一有效时刻保持一致，这在物理惯性较大的热力系统（HVAC）中是合理的假设。
\end{enumerate}

\section{探索性数据分析 (EDA)}

\subsection{时序特征分析}
建筑能耗数据具有显著的时间依赖性：
\begin{itemize}
    \item \textbf{日周期}：白天（9:00--18:00）负载显著高于夜间，反映办公作息特点；
    \item \textbf{周周期}：工作日负载均值较周末高约20\%--30\%；
    \item \textbf{时间聚合}：使用Pandas的\texttt{resample('1H')}将分钟级数据聚合至小时级，平滑噪声。
\end{itemize}

\subsection{相关性分析}
相关性分析表明，负载与温度呈显著正相关（Spearman相关系数较高），说明高温时段空调制冷需求增加推高了总负载。这一发现支持将 \texttt{Temperature} 作为核心预测特征。

\subsection{示例：Python 代码片段}

为了实现上述清洗与聚合逻辑，系统采用了 Python Pandas 库进行高效处理。以下代码片段展示了自动识别功率列并进行重采样的核心逻辑：

\begin{lstlisting}[language=Python, caption=数据预处理核心代码]
def preprocess_data(df):
    # 1. 自动识别所有包含 'kW' 的功率列
    power_columns = [col for col in df.columns if 'kW' in col]
    
    # 2. 按行求和生成楼层总负载
    df['Total_Load'] = df[power_columns].sum(axis=1)
    
    # 3. 设置时间索引并按小时重采样 (求均值)
    df = df.set_index('Date')
    hourly_df = df.resample('1H').mean()
    
    # 4. 缺失值前向填充
    hourly_df = hourly_df.ffill()
    
    return hourly_df
\end{lstlisting}

如图~\ref{fig:etl_pipeline}所示，完整的预处理流程确保了数据从原始 CSV 到结构化特征矩阵的高效流转。

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[
            node distance=1.2cm,
            box/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
            arrow/.style={->, >=stealth, thick}
        ]
        \node[box] (raw) {原始 CSV};
        \node[box, right=of raw] (agg) {功率聚合};
        \node[box, right=of agg] (merge) {楼层合并};
        \node[box, below=of merge] (resample) {小时重采样};
        \node[box, left=of resample] (feature) {特征工程};
        \node[box, left=of feature] (output) {输出数据};

        \draw[arrow] (raw) -- (agg);
        \draw[arrow] (agg) -- (merge);
        \draw[arrow] (merge) -- (resample);
        \draw[arrow] (resample) -- (feature);
        \draw[arrow] (feature) -- (output);
    \end{tikzpicture}
    \caption{数据预处理 ETL 流程}
    \label{fig:etl_pipeline}
\end{figure}
