"""
æœºå™¨å­¦ä¹ æœåŠ¡æ¨¡å— - èƒ½æºè´Ÿè½½é¢„æµ‹
ML Service Module for Energy Load Prediction

æ”¯æŒå¤šç§æ¨¡å‹ï¼šRandomForestã€LightGBMã€XGBoost
è‡ªåŠ¨æ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import joblib
import os
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings

warnings.filterwarnings('ignore')

# å¯é€‰ä¾èµ–ï¼šLightGBM å’Œ XGBoost
try:
    from lightgbm import LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ RandomForest")

try:
    from xgboost import XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ RandomForest")


from config import Config

class EnergyPredictor:
    """
    èƒ½æºè´Ÿè½½é¢„æµ‹å™¨
    
    ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹æœªæ¥24å°æ—¶çš„èƒ½æºè´Ÿè½½
    æ”¯æŒæ¨¡å‹è®­ç»ƒã€ä¿å­˜ã€åŠ è½½å’Œæ¨ç†
    """
    
    def __init__(self, model_path: str = None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œä¸»è¦ä» Firebase Storage åŠ è½½
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        self.script_dir = Path(__file__).parent  # services ç›®å½•
        self.back_dir = self.script_dir.parent  # back ç›®å½•
        
        # Firebase Storage æ¨¡å‹è·¯å¾„ï¼ˆä¸»è¦å­˜å‚¨ä½ç½®ï¼‰
        self.firebase_model_path = 'models/rf_model.joblib'
        
        # æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„ï¼ˆéƒ¨ç½²åŒ…ä¸­è‡ªå¸¦çš„æ¨¡å‹ï¼Œä»…ç”¨äºé¦–æ¬¡åŠ è½½ï¼‰
        if model_path:
            self.local_model_path = Path(model_path)
        else:
            self.local_model_path = self.back_dir / 'models' / 'rf_model.joblib'
        
        # åˆå§‹åŒ– StorageService
        from services.storage_service import StorageService
        self.storage_service = StorageService()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model: Optional[RandomForestRegressor] = None
        
        # åŸºç¡€ç‰¹å¾åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.base_feature_columns = [
            'Hour', 'DayOfWeek', 'Temperature', 'Price',
            'Lag_1h', 'Lag_24h', 'Lag_168h',
            'Rolling_Mean_6h', 'Rolling_Std_6h', 'Rolling_Mean_24h',
            'Temp_x_Hour', 'Lag24_x_DayOfWeek'
        ]
        
        # å¢å¼ºç‰¹å¾åˆ—è¡¨ï¼ˆæ–°å¢çš„æ—¶é—´ç‰¹å¾ï¼‰
        self.enhanced_feature_columns = [
            # åŸºç¡€æ—¶é—´ç‰¹å¾
            'Month', 'Season', 'IsWeekend', 'IsHoliday', 'DayOfMonth', 'WeekOfYear',
            # å¢å¼ºäº¤äº’ç‰¹å¾
            'Temp_x_Season', 'Lag24_x_IsWeekend', 'Hour_x_IsHoliday',
            # å‘¨æœŸç¼–ç ç‰¹å¾
            'Month_Sin', 'Month_Cos', 'Hour_Sin', 'Hour_Cos'
        ]
        
        # å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ï¼ˆåˆå§‹åŒ–ä¸ºåŸºç¡€ç‰¹å¾ï¼‰
        self.feature_columns = self.base_feature_columns.copy()
        self.target_column = 'Site_Load'
        
        print(f"ğŸ“ Firebase Storage æ¨¡å‹è·¯å¾„: {self.firebase_model_path}")
        print(f"ğŸ“ æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„: {self.local_model_path}")
    
    def _load_feature_columns_from_metadata(self):
        """
        ä»æ¨¡å‹å…ƒæ•°æ®ä¸­åŠ è½½ç‰¹å¾åˆ—è¡¨
        ç¡®ä¿é¢„æµ‹æ—¶ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        """
        try:
            metadata = self.get_model_metadata()
            if metadata and 'feature_engineering' in metadata:
                fe_info = metadata['feature_engineering']
                if fe_info.get('use_enhanced', False):
                    # æ¨¡å‹ä½¿ç”¨äº†å¢å¼ºç‰¹å¾ï¼Œæ›´æ–°ç‰¹å¾åˆ—è¡¨
                    enhanced_list = fe_info.get('enhanced_features_list', [])
                    if enhanced_list:
                        self.feature_columns = self.base_feature_columns.copy()
                        self.feature_columns.extend(enhanced_list)
                        print(f"   âœ“ å·²ä»å…ƒæ•°æ®æ¢å¤ç‰¹å¾åˆ—è¡¨: {len(self.feature_columns)} ä¸ªç‰¹å¾")
                        print(f"     (åŸºç¡€: {len(self.base_feature_columns)}, å¢å¼º: {len(enhanced_list)})")
                else:
                    # æ¨¡å‹åªä½¿ç”¨åŸºç¡€ç‰¹å¾
                    self.feature_columns = self.base_feature_columns.copy()
                    print(f"   âœ“ æ¨¡å‹ä½¿ç”¨åŸºç¡€ç‰¹å¾: {len(self.feature_columns)} ä¸ª")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°ç‰¹å¾å…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€ç‰¹å¾")
                
            # åŠ è½½è®­ç»ƒé…ç½® (Log Transform)
            if metadata and 'training_config' in metadata:
                config = metadata['training_config']
                self.use_log_transform = config.get('use_log_transform', False)
                if self.use_log_transform:
                    print(f"   âœ“ æ¨¡å‹ä½¿ç”¨ Log1p å˜æ¢ï¼Œé¢„æµ‹æ—¶å°†è‡ªåŠ¨è¿˜åŸ")
            else:
                self.use_log_transform = False
                
        except Exception as e:
            print(f"   âš ï¸  åŠ è½½ç‰¹å¾å…ƒæ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€ç‰¹å¾")
            self.use_log_transform = False
    
    def _get_model_type_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹çš„ç±»å‹åç§°"""
        if self.model is None:
            return 'Unknown'
        
        model_class = type(self.model).__name__
        name_map = {
            'RandomForestRegressor': 'Random Forest Regressor',
            'LGBMRegressor': 'LightGBM Regressor',
            'XGBRegressor': 'XGBoost Regressor'
        }
        return name_map.get(model_class, model_class)
    
    def _create_model(self, model_type: str, n_estimators: int = 100, random_state: int = 42):
        """
        åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('randomforest', 'lightgbm', 'xgboost')
            n_estimators: æ ‘çš„æ•°é‡
            random_state: éšæœºç§å­
            
        Returns:
            (model, hyperparameters) å…ƒç»„
        """
        model_type = model_type.lower()
        
        if model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            model = LGBMRegressor(
                n_estimators=n_estimators,
                learning_rate=0.05,
                max_depth=15,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
                verbose=-1
            )
            params = {
                'n_estimators': n_estimators,
                'learning_rate': 0.05,
                'max_depth': 15,
                'num_leaves': 31
            }
        elif model_type == 'xgboost' and XGBOOST_AVAILABLE:
            model = XGBRegressor(
                n_estimators=n_estimators,
                learning_rate=0.05,
                max_depth=10,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
                verbosity=0
            )
            params = {
                'n_estimators': n_estimators,
                'learning_rate': 0.05,
                'max_depth': 10
            }
        else:
            # é»˜è®¤ä½¿ç”¨ RandomForest
            model = RandomForestRegressor(
                n_estimators=n_estimators,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=random_state,
                n_jobs=-1,
                verbose=0
            )
            params = {
                'n_estimators': n_estimators,
                'max_depth': 20,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            }
        
        return model, params
    
    def _tune_model(self, model_type: str, X_train, y_train, cv_split, n_iter: int = 20, random_state: int = 42):
        """
        ä½¿ç”¨ RandomizedSearchCV å¯¹æŒ‡å®šæ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
        """
        model_type = model_type.lower()
        estimator = None
        param_dist = {}
        
        if model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            estimator = LGBMRegressor(random_state=random_state, n_jobs=-1, verbose=-1)
            param_dist = {
                'n_estimators': [100, 200, 300, 500],
                'learning_rate': [0.01, 0.03, 0.05, 0.1],
                'num_leaves': [31, 50, 70, 100],
                'max_depth': [10, 15, 20, -1],
                'subsample': [0.7, 0.8, 0.9],
                'colsample_bytree': [0.7, 0.8, 0.9]
            }
        elif model_type == 'xgboost' and XGBOOST_AVAILABLE:
            estimator = XGBRegressor(random_state=random_state, n_jobs=-1, verbosity=0)
            param_dist = {
                'n_estimators': [100, 200, 300, 500],
                'learning_rate': [0.01, 0.03, 0.05, 0.1],
                'max_depth': [3, 6, 10, 15],
                'subsample': [0.7, 0.8, 0.9],
                'colsample_bytree': [0.7, 0.8, 0.9],
                'gamma': [0, 0.1, 0.2]
            }
        else:
            # RandomForest
            estimator = RandomForestRegressor(random_state=random_state, n_jobs=-1)
            param_dist = {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 20, 30, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            }
            
        print(f"   ğŸ” æ­£åœ¨ä¸º {model_type} æœç´¢æœ€ä½³å‚æ•° (iter={n_iter})...")
        
        search = RandomizedSearchCV(
            estimator=estimator,
            param_distributions=param_dist,
            n_iter=n_iter,
            cv=cv_split,
            scoring='neg_mean_absolute_error',
            random_state=random_state,
            n_jobs=-1,
            verbose=0
        )
        
        search.fit(X_train, y_train)
        print(f"      âœ“ æœ€ä½³å‚æ•°: {search.best_params_}")
        return search.best_estimator_, search.best_params_

    def _auto_select_best_model(
        self, 
        X_train, y_train, 
        X_test, y_test, 
        random_state: int = 42,
        use_time_series_cv: bool = True,
        n_splits: int = 5,
        perform_tuning: bool = True  # æ–°å¢å‚æ•°
    ) -> tuple:
        """
        è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆæ”¯æŒæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼‰
        
        æ¯”è¾ƒå¤šç§æ¨¡å‹é…ç½®ï¼Œé€‰æ‹©æµ‹è¯•é›† MAE æœ€ä½çš„
        
        Args:
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_test, y_test: æµ‹è¯•æ•°æ®
            random_state: éšæœºç§å­
            use_time_series_cv: æ˜¯å¦ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆé»˜è®¤ Trueï¼‰
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆé»˜è®¤ 5ï¼‰
            
        Returns:
            (best_model, best_params, selection_info) å…ƒç»„
        """
        candidates = []
        results = {}
        cv_details = {}
        
        if perform_tuning:
            # å¦‚æœå¯ç”¨è°ƒä¼˜ï¼Œæ­¤æ—¶æˆ‘ä»¬ä¸ä½¿ç”¨é¢„è®¾çš„é…ç½®ï¼Œè€Œæ˜¯é’ˆå¯¹æ¯ä¸ªæ¨¡å‹ç±»å‹è¿è¡Œæœç´¢
            # ä½†ä¸ºäº†ä¿æŒä»£ç ç»“æ„ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªå¾…è°ƒä¼˜ç±»å‹åˆ—è¡¨
            # è¿™é‡Œçš„ (DisplayName, type, init_n_estimators) ä»…ç”¨äºå…ƒç»„è§£åŒ…ï¼Œn_estimators ä¼šè¢«å¿½ç•¥
            tuning_targets = [('RandomForest', 'randomforest', 100)]
            if LIGHTGBM_AVAILABLE:
                tuning_targets.append(('LightGBM', 'lightgbm', 100))
            if XGBOOST_AVAILABLE:
                tuning_targets.append(('XGBoost', 'xgboost', 100))
            
            model_configs = tuning_targets # æ›¿æ¢ä¸ºè°ƒä¼˜ç›®æ ‡
        else:
            # åŸæœ‰çš„å›ºå®šé…ç½®æ¨¡å¼
            model_configs = [
                ('RandomForest', 'randomforest', 150),
                ('RandomForest_200', 'randomforest', 200),
            ]
            
            # å¦‚æœ LightGBM å¯ç”¨ï¼Œæ·»åŠ åˆ°å€™é€‰
            if LIGHTGBM_AVAILABLE:
                model_configs.extend([
                    ('LightGBM', 'lightgbm', 200),
                    ('LightGBM_300', 'lightgbm', 300),
                ])
            
            # å¦‚æœ XGBoost å¯ç”¨ï¼Œæ·»åŠ åˆ°å€™é€‰
            if XGBOOST_AVAILABLE:
                model_configs.extend([
                    ('XGBoost', 'xgboost', 200),
                    ('XGBoost_300', 'xgboost', 300),
                ])
        
        print(f"   è¯„ä¼° {len(model_configs)} ç§æ¨¡å‹é…ç½®...")
        if use_time_series_cv:
            print(f"   ğŸ“Š ä½¿ç”¨ TimeSeriesSplit äº¤å‰éªŒè¯ ({n_splits} æŠ˜)")
        
        baseline_mae = None
        best_mae = float('inf')
        best_model = None
        best_params = None
        best_name = None
        
        # ã€ä¿®å¤ã€‘æ—¶é—´åºåˆ—äº¤å‰éªŒè¯åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œï¼Œæµ‹è¯•é›†ä¿æŒç‹¬ç«‹ç”¨äºæœ€ç»ˆè¯„ä¼°
        # è¿™é¿å…äº†æµ‹è¯•é›†å‚ä¸æ¨¡å‹é€‰æ‹©å¯¼è‡´çš„æ•°æ®æ³„æ¼
        if use_time_series_cv:
            # CV åªåœ¨è®­ç»ƒé›†ä¸Šåšï¼Œä¸å†æŠŠæµ‹è¯•é›†æ‹¼è¿›å»
            tscv = TimeSeriesSplit(n_splits=n_splits)
            print(f"   ğŸ“Š TimeSeriesSplit CV ä»…åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ ({len(X_train)} æ ·æœ¬)")
        
        # å­˜å‚¨æ¯ç§ç±»å‹çš„æœ€ä½³æ¨¡å‹ï¼Œç”¨äºé›†æˆ
        best_estimators = {}
        
        for name, model_type, n_estimators in model_configs:
            try:
                model = None
                params = None
                
                if perform_tuning:
                    # ä½¿ç”¨è¶…å‚æ•°æœç´¢
                    # æ³¨æ„ï¼šå¤ç”¨ tscv ä½œä¸º cv_split
                    # å¦‚æœæœªå¯ç”¨ TimeSeriesCVï¼Œåˆ™ä½¿ç”¨ KFold æˆ–è€…é»˜è®¤çš„ 5æŠ˜
                    cv_to_use = tscv if use_time_series_cv else 5
                    
                    # è°ƒä¼˜åçš„åå­—åŠ ä¸Š "Tuned" åç¼€
                    name = f"{name}_Tuned"
                    model, params = self._tune_model(model_type, X_train, y_train, cv_to_use, n_iter=15, random_state=random_state)
                else:
                    # ä½¿ç”¨å›ºå®šé…ç½®
                    print(f"   - è®­ç»ƒ {name}...", end=' ')
                    model, params = self._create_model(model_type, n_estimators, random_state)
                
                if use_time_series_cv:
                    # ã€ä¿®å¤ã€‘æ—¶é—´åºåˆ—äº¤å‰éªŒè¯åªåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œ
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_train):
                        X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
                        
                        model_cv, _ = self._create_model(model_type, n_estimators, random_state)
                        model_cv.fit(X_cv_train, y_cv_train)
                        y_cv_pred = model_cv.predict(X_cv_val)
                        
                        # å¦‚æœå¯ç”¨äº†å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è¿˜åŸé¢„æµ‹å€¼
                        if hasattr(self, 'use_log_transform') and self.use_log_transform:
                            y_cv_val = np.expm1(y_cv_val)
                            y_cv_pred = np.expm1(y_cv_pred)
                            
                        cv_scores.append(mean_absolute_error(y_cv_val, y_cv_pred))
                    
                    # è®¡ç®—äº¤å‰éªŒè¯å¹³å‡åˆ†
                    cv_mae = np.mean(cv_scores)
                    cv_std = np.std(cv_scores)
                    
                    # ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                    model.fit(X_train, y_train)
                    
                    # åœ¨ç‹¬ç«‹çš„æœªæ¥æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆä¸¥æ ¼æœªè§æ•°æ®ï¼‰
                    y_pred = model.predict(X_test)
                    
                    # å¦‚æœå¯ç”¨äº†å¯¹æ•°å˜æ¢ï¼Œéœ€è¦è¿˜åŸé¢„æµ‹å€¼
                    if hasattr(self, 'use_log_transform') and self.use_log_transform:
                         # y_test æ˜¯åŸå§‹å€¼ï¼ˆå¦‚æœä¸æ”¹åŠ¨ y_test çš„è¯ï¼‰ï¼Œè¿™é‡Œéœ€è¦ç¡®è®¤ split æ—¶ y_test æ˜¯å¦è¢« log
                         # æˆ‘ä»¬çš„ç­–ç•¥æ˜¯ï¼šåªå¯¹è®­ç»ƒé›†çš„ y è¿›è¡Œ logï¼Œæµ‹è¯•é›†çš„ y ä¿æŒåŸæ ·æˆ–è€…åå˜æ¢
                         # ä¸ºç»Ÿä¸€å¤„ç†ï¼Œå‡è®¾ fit æ—¶ y_train æ˜¯ log åçš„ï¼Œpredict è¾“å‡º log åçš„
                         y_pred = np.expm1(y_pred)
                         # y_test åœ¨ split å‰å¦‚æœæ²¡å˜ï¼Œé‚£å°±æ˜¯åŸå§‹å€¼ã€‚
                         # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ç¨ååœ¨ split å¤„åšå¤„ç†ã€‚
                    
                    test_mae = mean_absolute_error(y_test, y_pred)
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    
                    # ä½¿ç”¨äº¤å‰éªŒè¯ MAE ä½œä¸ºé€‰æ‹©ä¾æ®ï¼ˆæ›´å¯é ï¼‰
                    mae = cv_mae
                    rmse = test_rmse
                    
                    cv_details[name] = {
                        'cv_mae_mean': round(cv_mae, 2),
                        'cv_mae_std': round(cv_std, 2),
                        'cv_scores': [round(s, 2) for s in cv_scores],
                        'test_mae': round(test_mae, 2)  # è®°å½•ç‹¬ç«‹æµ‹è¯•é›†çš„ MAE
                    }
                    
                    print(f"CV_MAE={cv_mae:.2f}Â±{cv_std:.2f} kW, Test_MAE={test_mae:.2f} kW (ç‹¬ç«‹æœªæ¥æ•°æ®)")
                else:
                    # åŸæœ‰çš„ç®€å•è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                    
                    if hasattr(self, 'use_log_transform') and self.use_log_transform:
                        y_pred = np.expm1(y_pred)
                    
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    print(f"MAE={mae:.2f} kW")
                
                results[name] = {'mae': mae, 'rmse': rmse, 'model_type': model_type}
                candidates.append(name)
                
                # è®°å½•åŸºå‡†ï¼ˆç¬¬ä¸€ä¸ª RandomForestï¼‰
                if baseline_mae is None:
                    baseline_mae = mae
                
                # æ›´æ–°æœ€ä½³
                if mae < best_mae:
                    best_mae = mae
                    best_model = model
                    best_params = params
                    best_name = name
                    
            except Exception as e:
                print(f"å¤±è´¥: {str(e)}")
                continue
            
            # è®°å½•è¯¥ç±»å‹çš„æœ€ä½³æ¨¡å‹ç”¨äºé›†æˆ
            if model_type not in best_estimators or mae < best_estimators[model_type]['mae']:
                best_estimators[model_type] = {
                    'model': model,
                    'mae': mae
                }
        
        # å°è¯•é›†æˆå­¦ä¹  (VotingRegressor)
        if len(best_estimators) >= 2:
            try:
                print(f"   ğŸ¤ å°è¯•é›†æˆå­¦ä¹  (VotingRegressor)...", end=' ')
                # åˆ›å»º VotingRegressor
                estimators_list = [(k, v['model']) for k, v in best_estimators.items()]
                voting_model = VotingRegressor(estimators=estimators_list, n_jobs=-1)
                voting_name = "Voting_Ensemble"
                
                # è¯„ä¼° Voting æ¨¡å‹
                if use_time_series_cv:
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_train):
                        X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                        y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
                        
                        from sklearn.base import clone
                        model_cv = clone(voting_model)
                        model_cv.fit(X_cv_train, y_cv_train)
                        y_cv_pred = model_cv.predict(X_cv_val)
                        cv_scores.append(mean_absolute_error(y_cv_val, y_cv_pred))
                    
                    cv_mae = np.mean(cv_scores)
                    cv_std = np.std(cv_scores)
                    
                    voting_model.fit(X_train, y_train)
                    y_pred = voting_model.predict(X_test)
                    
                    if hasattr(self, 'use_log_transform') and self.use_log_transform:
                         y_pred = np.expm1(y_pred)
                         
                    test_mae = mean_absolute_error(y_test, y_pred)
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    
                    mae = cv_mae
                    rmse = test_rmse
                    
                    cv_details[voting_name] = {
                        'cv_mae_mean': round(cv_mae, 2),
                        'cv_mae_std': round(cv_std, 2),
                        'cv_scores': [round(s, 2) for s in cv_scores],
                        'test_mae': round(test_mae, 2)
                    }
                    print(f"CV_MAE={cv_mae:.2f}Â±{cv_std:.2f} kW, Test_MAE={test_mae:.2f} kW")
                else:
                    voting_model.fit(X_train, y_train)
                    y_pred = voting_model.predict(X_test)
                    
                    if hasattr(self, 'use_log_transform') and self.use_log_transform:
                        y_pred = np.expm1(y_pred)
                    
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    print(f"MAE={mae:.2f} kW")
                
                results[voting_name] = {'mae': mae, 'rmse': rmse, 'model_type': 'voting'}
                candidates.append(voting_name)
                
                if mae < best_mae:
                    best_mae = mae
                    best_model = voting_model
                    best_params = {'estimators': [k for k in best_estimators.keys()]}
                    best_name = voting_name
                    print(f"   âœ¨ é›†æˆæ¨¡å‹è¡¨ç°æœ€ä½³!")
                    
            except Exception as e:
                print(f"   âš ï¸ é›†æˆå­¦ä¹ å¤±è´¥: {str(e)}")

        # è®¡ç®—ç›¸å¯¹åŸºå‡†çš„æå‡
        improvement = 'N/A'
        if baseline_mae and baseline_mae > 0:
            improvement_pct = (baseline_mae - best_mae) / baseline_mae * 100
            improvement = f"{improvement_pct:.1f}%"
        
        print(f"\n   ğŸ† æœ€ä½³æ¨¡å‹: {best_name} (MAE={best_mae:.2f} kW)")
        if improvement != 'N/A' and improvement != '0.0%':
            print(f"   ğŸ“ˆ ç›¸æ¯”åŸºå‡†æå‡: {improvement}")
        
        selection_info = {
            'winner': best_name,
            'candidates_evaluated': candidates,
            'all_scores': {k: {'mae': round(v['mae'], 2), 'rmse': round(v['rmse'], 2)} 
                          for k, v in results.items()},
            'improvement': improvement,
            'validation_method': 'TimeSeriesSplit' if use_time_series_cv else 'HoldOut',
            'cv_folds': n_splits if use_time_series_cv else None,
            'cv_details': cv_details if use_time_series_cv else None
        }
        
        return best_model, best_params, selection_info
    
    def _get_price(self, hour: int) -> float:
        """
        æ ¹æ®å°æ—¶è¿”å›å³°è°·ç”µä»· (ä»é…ç½®è¯»å–)
        
        Args:
            hour: å°æ—¶ (0-23)
            
        Returns:
            ç”µä»· (å…ƒ/kWh)
        """
        schedule = Config.PRICE_SCHEDULE
        
        if hour in schedule['peak_hours_list']:
            return schedule['peak']
        elif hour in schedule['normal_hours_list']:
            return schedule['normal']
        else:
            return schedule['valley']
    
    def _save_model_metadata(self, metadata: dict) -> bool:
        """
        ä¿å­˜æ¨¡å‹å…ƒæ•°æ®åˆ° Firebase Storage (JSON æ–‡ä»¶)
        
        Args:
            metadata: æ¨¡å‹å…ƒæ•°æ®å­—å…¸
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import json
            from datetime import datetime
            
            # æ·»åŠ æ›´æ–°æ—¶é—´æˆ³
            metadata['updated_at'] = datetime.now().isoformat()
            
            # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            json_data = json.dumps(metadata, indent=2, ensure_ascii=False)
            
            # ä¸Šä¼ åˆ° Storage (ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿ Content-Type æ­£ç¡®)
            import tempfile
            metadata_path = 'models/model_metadata.json'
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
                f.write(json_data)
                temp_path = f.name
            
            try:
                with open(temp_path, 'rb') as f:
                    self.storage_service.bucket.blob(metadata_path).upload_from_file(
                        f, 
                        content_type='application/json'
                    )
            finally:
                import os
                os.unlink(temp_path)
            
            print(f"   âœ“ æ¨¡å‹å…ƒæ•°æ®å·²ä¿å­˜åˆ° Firebase Storage: {metadata_path}")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def get_model_metadata() -> Optional[dict]:
        """
        ä» Firebase Storage è·å–æ¨¡å‹å…ƒæ•°æ® (JSON æ–‡ä»¶)
        
        Returns:
            æ¨¡å‹å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        try:
            import json
            from services.storage_service import StorageService
            
            # åˆ›å»º Storage æœåŠ¡å®ä¾‹
            storage = StorageService()
            
            # ä¸‹è½½å…ƒæ•°æ® JSON
            metadata_path = 'models/model_metadata.json'
            json_bytes = storage.download_file(metadata_path)
            
            # è§£æ JSON
            metadata = json.loads(json_bytes.decode('utf-8'))
            return metadata
                
        except Exception as e:
            print(f"è·å–æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def _is_gae_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦åœ¨ GAE ç¯å¢ƒ"""
        return bool(os.getenv('GAE_ENV') or os.getenv('K_SERVICE'))

    def train_model(
        self, 
        data_path: str = None,
        n_estimators: int = 100,
        test_size: float = 0.2,
        random_state: int = 42,
        use_firebase_storage: bool = True,
        auto_select_model: bool = True,
        model_type: str = 'auto',
        use_enhanced_features: bool = True,
        use_time_series_cv: bool = True,
        cv_folds: int = 5,
        use_log_transform: bool = True,
        remove_outliers: bool = True,
        tune_hyperparameters: bool = True  # æ–°å¢æ§åˆ¶å¼€å…³ (é»˜è®¤å¼€å¯)
    ) -> Dict[str, float]:
        """
        è®­ç»ƒé¢„æµ‹æ¨¡å‹ï¼ˆæ”¯æŒè‡ªåŠ¨æ¨¡å‹é€‰æ‹©ã€å¢å¼ºç‰¹å¾å’Œæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼‰
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            n_estimators: æ ‘çš„æ•°é‡
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            use_firebase_storage: æ˜¯å¦ä» Firebase Storage ä¸‹è½½æ•°æ®
            auto_select_model: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
            model_type: æŒ‡å®šæ¨¡å‹ç±»å‹
            use_enhanced_features: æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰¹å¾
            use_time_series_cv: æ˜¯å¦ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            use_log_transform: æ˜¯å¦å¯¹ç›®æ ‡å˜é‡è¿›è¡Œ Log1p å˜æ¢ (é»˜è®¤ True)
            remove_outliers: æ˜¯å¦è‡ªåŠ¨è¿‡æ»¤å¼‚å¸¸å€¼ (é»˜è®¤ True)
            tune_hyperparameters: æ˜¯å¦å¯ç”¨è¶…å‚æ•°è‡ªåŠ¨æœç´¢ (é»˜è®¤ True) 
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹è®­ç»ƒèƒ½æºè´Ÿè½½é¢„æµ‹æ¨¡å‹")
        print("="*80 + "\n")
        
        temp_data_path = None
        
        try:
            # ä» Firebase Storage ä¸‹è½½æ•°æ®
            if use_firebase_storage:
                print("ğŸ“¥ ä» Firebase Storage ä¸‹è½½è®­ç»ƒæ•°æ®...")
                from services.storage_service import StorageService
                
                storage_service = StorageService()
                firebase_path = data_path or 'data/processed/cleaned_energy_data_all.csv'
                
                temp_data_path = storage_service.download_to_temp(firebase_path)
                
                if temp_data_path is None:
                    raise FileNotFoundError(f"æ— æ³•ä» Firebase Storage ä¸‹è½½æ•°æ®: {firebase_path}")
                
                data_path = temp_data_path
                print(f"   âœ“ æ•°æ®å·²ä¸‹è½½åˆ°: {data_path}")
            else:
                # æœ¬åœ°æ–‡ä»¶æ¨¡å¼ (å¼€å‘ç¯å¢ƒ)
                if data_path is None:
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
                    possible_paths = [
                        self.back_dir.parent / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
                        self.back_dir / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
                    ]
                    data_path = None
                    for path in possible_paths:
                        if path.exists():
                            data_path = path
                            break
                    if data_path is None:
                        data_path = possible_paths[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤
                else:
                    data_path = Path(data_path)
            
            # è¯»å–æ•°æ®
            print(f"ğŸ“– è¯»å–æ•°æ®: {data_path}")
            try:
                df = pd.read_csv(data_path, parse_dates=['Date'])
                print(f"   âœ“ æ•°æ®è¯»å–æˆåŠŸ: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
            except FileNotFoundError:
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            except Exception as e:
                raise Exception(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            
            # ================================================================
            # ã€ä¿®å¤ã€‘æ™ºèƒ½ç‰¹å¾æ£€æŸ¥ä¸è‡ªåŠ¨è¡¥ç®—
            # åˆ†å±‚æ£€æŸ¥ï¼šç¡¬å¿…éœ€åˆ— vs å¯è¡¥ç®—çš„ engineered ç‰¹å¾
            # ================================================================
            
            # ç¡¬å¿…éœ€åˆ—ï¼šå¿…é¡»å­˜åœ¨ï¼Œæ— æ³•è‡ªåŠ¨ç”Ÿæˆ
            hard_required = ['Date', 'Site_Load', 'Temperature']
            missing_hard = [col for col in hard_required if col not in df.columns]
            if missing_hard:
                raise ValueError(f"æ•°æ®ç¼ºå°‘æ ¸å¿ƒå¿…éœ€åˆ—ï¼ˆæ— æ³•è‡ªåŠ¨ç”Ÿæˆï¼‰: {missing_hard}")
            
            # å¯è¡¥ç®—çš„åŸºç¡€æ—¶é—´ç‰¹å¾
            time_features = ['Hour', 'DayOfWeek', 'Price']
            missing_time = [col for col in time_features if col not in df.columns]
            
            # å¯è¡¥ç®—çš„ Lag/Rolling ç‰¹å¾
            engineered_features = ['Lag_1h', 'Lag_24h', 'Lag_168h', 
                                   'Rolling_Mean_6h', 'Rolling_Std_6h', 'Rolling_Mean_24h',
                                   'Temp_x_Hour', 'Lag24_x_DayOfWeek']
            missing_engineered = [col for col in engineered_features if col not in df.columns]
            
            # æ£€æŸ¥å¢å¼ºç‰¹å¾æ˜¯å¦ç¼ºå¤±
            missing_enhanced_time = []
            if use_enhanced_features:
                 enhanced_time_cols = ['Month', 'Season', 'IsWeekend', 'IsHoliday']
                 missing_enhanced_time = [c for c in enhanced_time_cols if c not in df.columns]

            # å¦‚æœæœ‰ç¼ºå¤±çš„å¯è¡¥ç®—ç‰¹å¾ï¼Œè‡ªåŠ¨è°ƒç”¨ EnergyDataProcessor è¡¥ç®—
            if missing_time or missing_engineered or missing_enhanced_time:
                print(f"\nğŸ”§ æ£€æµ‹åˆ°ç¼ºå¤±ç‰¹å¾ï¼Œè‡ªåŠ¨è¡¥ç®—ä¸­...")
                if missing_time:
                    print(f"   - ç¼ºå¤±æ—¶é—´ç‰¹å¾: {missing_time}")
                if missing_engineered:
                    print(f"   - ç¼ºå¤±å·¥ç¨‹ç‰¹å¾: {missing_engineered}")
                
                from services.data_processor import EnergyDataProcessor
                processor = EnergyDataProcessor()
                
                # ç¡®ä¿ Date æ˜¯ datetime ç±»å‹
                if df['Date'].dtype == 'object':
                    df['Date'] = pd.to_datetime(df['Date'])
                
                # è¡¥ç®—æ—¶é—´ç‰¹å¾
                if missing_time:
                    if 'Hour' not in df.columns:
                        df['Hour'] = df['Date'].dt.hour
                    if 'DayOfWeek' not in df.columns:
                        df['DayOfWeek'] = df['Date'].dt.dayofweek
                    if 'Price' not in df.columns:
                        df = processor.add_price_feature(df)
                
                # è¡¥ç®—å¢å¼ºæ—¶é—´ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if use_enhanced_features:
                    enhanced_time_cols = ['Month', 'Season', 'IsWeekend', 'IsHoliday']
                    missing_enhanced_time = [c for c in enhanced_time_cols if c not in df.columns]
                    if missing_enhanced_time:
                        df = processor.add_enhanced_time_features(df)
                        print(f"   âœ“ å·²è¡¥ç®—å¢å¼ºæ—¶é—´ç‰¹å¾")
                
                # è¡¥ç®— Lag/Rolling/Interaction ç‰¹å¾
                if missing_engineered:
                    df = processor.add_advanced_features(df, dropna=False, use_enhanced=use_enhanced_features)
                    print(f"   âœ“ å·²è¡¥ç®— Lag/Rolling/Interaction ç‰¹å¾")
                
                print(f"   âœ“ ç‰¹å¾è¡¥ç®—å®Œæˆï¼Œå½“å‰åˆ—æ•°: {len(df.columns)}")
            
            # å¤„ç†ç¼ºå¤±å€¼
            print(f"\nğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")
            # æ£€æŸ¥æ ¸å¿ƒåˆ—çš„ç¼ºå¤±å€¼
            core_cols = hard_required + time_features
            null_counts = df[core_cols].isnull().sum()
            if null_counts.sum() > 0:
                print(f"   âš ï¸  å‘ç°ç¼ºå¤±å€¼:")
                for col, count in null_counts[null_counts > 0].items():
                    print(f"      - {col}: {count} ä¸ª")
                
                # å¯¹äº Temperatureï¼Œä½¿ç”¨å‡å€¼å¡«å……
                if 'Temperature' in null_counts and null_counts['Temperature'] > 0:
                    # ä½¿ç”¨çº¿æ€§æ’å€¼å¡«å……æ¸©åº¦ç¼ºå¤±å€¼ï¼ˆæ›´é€‚åˆæ—¶é—´åºåˆ—ï¼‰
                    df['Temperature'] = df['Temperature'].interpolate(method='linear', limit_direction='both')
                    # å¦‚æœä»æœ‰ç¼ºå¤±ï¼ˆä¾‹å¦‚å¼€å¤´æˆ–ç»“å°¾ï¼‰ï¼Œä½¿ç”¨å‡å€¼å…œåº•
                    if df['Temperature'].isnull().sum() > 0:
                         df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)
                    print(f"   âœ“ Temperature ç¼ºå¤±å€¼å·²ä½¿ç”¨çº¿æ€§æ’å€¼å¡«å……")
            else:
                print(f"   âœ“ æ— æ ¸å¿ƒåˆ—ç¼ºå¤±å€¼")
            
            # ================================================================
            # åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼ˆæ£€æµ‹æ•°æ®ä¸­å¯ç”¨çš„å¢å¼ºç‰¹å¾ï¼‰
            # ================================================================
            print(f"\nğŸ”§ æ£€æµ‹å¯ç”¨ç‰¹å¾...")
            
            # é¦–å…ˆä½¿ç”¨åŸºç¡€ç‰¹å¾
            available_features = [col for col in self.base_feature_columns if col in df.columns]
            print(f"   âœ“ åŸºç¡€ç‰¹å¾: {len(available_features)}/{len(self.base_feature_columns)}")
            
            # å¦‚æœå¯ç”¨å¢å¼ºç‰¹å¾ï¼Œæ£€æµ‹å¹¶æ·»åŠ 
            enhanced_features_used = []
            if use_enhanced_features:
                for col in self.enhanced_feature_columns:
                    if col in df.columns:
                        available_features.append(col)
                        enhanced_features_used.append(col)
                
                if enhanced_features_used:
                    print(f"   âœ“ å¢å¼ºç‰¹å¾: {len(enhanced_features_used)} ä¸ª")
                    print(f"     {enhanced_features_used}")
                else:
                    print(f"   âš ï¸  æ•°æ®ä¸­æœªæ‰¾åˆ°å¢å¼ºç‰¹å¾ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")
            
            # æ›´æ–°å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
            self.feature_columns = available_features
            print(f"   ğŸ“Š æ€»è®¡ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
                
            # æ¸…é™¤å› ç‰¹å¾å·¥ç¨‹ (Lag/Rolling) äº§ç”Ÿçš„ NaN è¡Œ
            # è¿™äº›è¡Œé€šå¸¸ä½äºæ•°æ®é›†å¤´éƒ¨
            before_drop = len(df)
            df.dropna(inplace=True)
            after_drop = len(df)
            if before_drop != after_drop:
                print(f"   âœ‚ï¸  å·²åˆ é™¤ {before_drop - after_drop} è¡ŒåŒ…å« NaN çš„æ ·æœ¬ (Lag/Rolling start-up)")
            
            # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
            print(f"\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
            X = df[self.feature_columns].copy()
            y = df[self.target_column].copy()
            
            print(f"   - ç‰¹å¾åˆ—: {self.feature_columns}")
            print(f"   - ç›®æ ‡å˜é‡: {self.target_column}")
            print(f"   - æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            # ã€ä¿®å¤ã€‘æ—¶é—´åºåˆ—æ•°æ®å¿…é¡»æŒ‰æ—¶é—´é¡ºåºæ‹†åˆ†ï¼Œé¿å…æ•°æ®æ³„æ¼
            print(f"\nâœ‚ï¸  åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒé›†: {int((1-test_size)*100)}%, æµ‹è¯•é›†: {int(test_size*100)}%)...")
            
            if use_time_series_cv:
                # æ—¶é—´åºåˆ—æ¨¡å¼ï¼šæŒ‰æ—¶é—´é¡ºåºæ‹†åˆ†ï¼Œæµ‹è¯•é›†ä¸ºä¸¥æ ¼æœªæ¥æ•°æ®
                # ç¡®ä¿æ•°æ®æŒ‰ Date æ’åºï¼ˆå¦‚æœ Date åˆ—å­˜åœ¨ï¼‰
                if 'Date' in df.columns:
                    sort_idx = df['Date'].argsort()
                    X = X.iloc[sort_idx].reset_index(drop=True)
                    y = y.iloc[sort_idx].reset_index(drop=True)
                    print(f"   âœ“ å·²æŒ‰æ—¶é—´é¡ºåºæ’åºæ•°æ®")
                
                # æŒ‰æ—¶é—´é¡ºåºæ‹†åˆ†ï¼šæœ€å test_size æ¯”ä¾‹ä½œä¸ºä¸¥æ ¼æœªæ¥ holdout
                split_idx = int(len(X) * (1 - test_size))
                X_train = X.iloc[:split_idx].copy()
                X_test = X.iloc[split_idx:].copy()
                y_train = y.iloc[:split_idx].copy()
                y_test = y.iloc[split_idx:].copy()
                print(f"   âœ“ æ—¶é—´åºåˆ—æ‹†åˆ†ï¼šæµ‹è¯•é›†ä¸ºä¸¥æ ¼æœªæ¥æ•°æ® (é¿å…æ•°æ®æ³„æ¼)")
            else:
                # éæ—¶é—´åºåˆ—æ¨¡å¼ï¼šä¿æŒåŸæœ‰éšæœºæ‹†åˆ†è¡Œä¸ºï¼ˆå‘åå…¼å®¹ï¼‰
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=test_size, random_state=random_state
                )
                print(f"   âš ï¸  éšæœºæ‹†åˆ†æ¨¡å¼ (ä»…ç”¨äºéæ—¶é—´åºåˆ—æ•°æ®)")
            
            print(f"   - è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
            print(f"   - æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
            
            # è®°å½•é…ç½®çŠ¶æ€
            self.use_log_transform = use_log_transform
            
            # ================================================================
            # å¼‚å¸¸å€¼è¿‡æ»¤ (Anomaly Detection) - ä»…é’ˆå¯¹è®­ç»ƒé›†
            # ================================================================
            if remove_outliers:
                print(f"\nğŸ§¹ æ‰§è¡Œå¼‚å¸¸å€¼è¿‡æ»¤ (IQR)...")
                # è®¡ç®— IQR
                Q1 = y_train.quantile(0.25)
                Q3 = y_train.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # è¿‡æ»¤ç´¢å¼•
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åŒæ—¶éœ€è¦è¿‡æ»¤ X_train å’Œ y_train
                mask = (y_train >= lower_bound) & (y_train <= upper_bound)
                outliers_count = len(y_train) - mask.sum()
                
                if outliers_count > 0:
                     X_train = X_train[mask]
                     y_train = y_train[mask]
                     print(f"   âœ“ å·²ç§»é™¤ {outliers_count} ä¸ªå¼‚å¸¸æ ·æœ¬")
                     print(f"     é˜ˆå€¼åŒºé—´: [{lower_bound:.2f}, {upper_bound:.2f}]")
                else:
                     print(f"   âœ“ æœªå‘ç°æ˜¾è‘—å¼‚å¸¸å€¼")
            
            # ================================================================
            # ç›®æ ‡å˜é‡å¯¹æ•°å˜æ¢ (Log Transform)
            # ================================================================
            if use_log_transform:
                print(f"\nğŸ“‰æ‰§è¡Œ Log1p å˜æ¢...")
                # æ£€æŸ¥è´Ÿå€¼
                if (y_train < 0).any():
                     print(f"   âš ï¸  y_train åŒ…å«è´Ÿå€¼ï¼Œæ— æ³•è¿›è¡Œ Log å˜æ¢ï¼Œè‡ªåŠ¨ç¦ç”¨")
                     self.use_log_transform = False
                else:
                     y_train = np.log1p(y_train)
                     # y_test ä¸å˜æ¢ï¼Œä¿æŒåŸå§‹æ¯”ä¾‹ç”¨äºè¯„ä¼°
                     print(f"   âœ“ y_train å·²è¿›è¡Œ np.log1p å˜æ¢")

            # ================================================================
            # è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒ
            # ================================================================
            if auto_select_model and model_type == 'auto':
                print(f"\nğŸ¤– è‡ªåŠ¨æ¨¡å‹é€‰æ‹©æ¨¡å¼...")
                best_model, best_params, selection_info = self._auto_select_best_model(
                    X_train, y_train, X_test, y_test, random_state,
                    use_time_series_cv=use_time_series_cv,
                    n_splits=cv_folds,
                    perform_tuning=tune_hyperparameters
                )
                self.model = best_model
                selected_model_type = selection_info['winner']
                hyperparameters = best_params
            else:
                # æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç±»å‹
                print(f"\nğŸŒ² è®­ç»ƒ {model_type} æ¨¡å‹...")
                self.model, hyperparameters = self._create_model(
                    model_type, n_estimators, random_state
                )
                self.model.fit(X_train, y_train)
                selected_model_type = model_type
                selection_info = {'winner': model_type, 'candidates_evaluated': [model_type]}
            
            print(f"   âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ! (ç±»å‹: {selected_model_type})")
            
            # è¯„ä¼°æ¨¡å‹
            print(f"\nğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è®­ç»ƒé›†é¢„æµ‹
            y_train_pred = self.model.predict(X_train)
            
            # è¿˜åŸ
            if self.use_log_transform:
                y_train = np.expm1(y_train)
                y_train_pred = np.expm1(y_train_pred)
                
            train_mae = mean_absolute_error(y_train, y_train_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            
            # æµ‹è¯•é›†é¢„æµ‹
            y_test_pred = self.model.predict(X_test)
            
            if self.use_log_transform:
                y_test_pred = np.expm1(y_test_pred)
                
            test_mae = mean_absolute_error(y_test, y_test_pred)
            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
            
            # è®¡ç®— RÂ² Score (æµ‹è¯•é›†)
            test_r2 = r2_score(y_test, y_test_pred)
            
            # è®¡ç®— MAPE (æµ‹è¯•é›†) - Mean Absolute Percentage Error
            # é¿å…åˆ†æ¯ä¸º 0
            mask = y_test.values != 0
            if np.sum(mask) > 0:
                test_mape = np.mean(np.abs((y_test.values[mask] - y_test_pred[mask]) / y_test.values[mask])) * 100
            else:
                test_mape = 0.0
            
            print(f"\n   è®­ç»ƒé›†æ€§èƒ½:")
            print(f"      - MAE:  {train_mae:.2f} kW")
            print(f"      - RMSE: {train_rmse:.2f} kW")
            
            print(f"\n   æµ‹è¯•é›†æ€§èƒ½:")
            print(f"      - MAE:  {test_mae:.2f} kW")
            print(f"      - RMSE: {test_rmse:.2f} kW")
            print(f"      - RÂ²:   {test_r2:.4f}")
            print(f"      - MAPE: {test_mape:.2f}%")

            # ------------------------------------------------------------
            # éªŒè¯ä¿¡æ¯æ±‡æ€» (ç”¨äºå…ƒæ•°æ®å’Œå‰ç«¯å±•ç¤º)
            # ------------------------------------------------------------
            validation_summary = {
                'method': 'TimeSeriesSplit' if use_time_series_cv else 'HoldOut',
                'cv_folds': cv_folds if use_time_series_cv else None,
                'holdout_mae': float(test_mae),
                'holdout_rmse': float(test_rmse),
                'holdout_r2': float(test_r2),
                'holdout_mape': float(test_mape / 100),  # ç»Ÿä¸€ä¸ºå°æ•°
            }

            # å¦‚æœæœ‰äº¤å‰éªŒè¯è¯¦æƒ…ï¼Œè®°å½•å½“å‰èƒœå‡ºæ¨¡å‹çš„å‡å€¼å’Œæ³¢åŠ¨
            if auto_select_model and model_type == 'auto' and selection_info:
                winner_name = selection_info.get('winner')
                cv_detail = (selection_info.get('cv_details') or {}).get(winner_name, {})
                if cv_detail:
                    validation_summary.update({
                        'cv_mae_mean': cv_detail.get('cv_mae_mean'),
                        'cv_mae_std': cv_detail.get('cv_mae_std'),
                        'cv_scores': cv_detail.get('cv_scores')
                    })

            # æ•°æ®è¦†ç›–èŒƒå›´
            data_coverage = None
            if 'Date' in df.columns:
                date_min = df['Date'].min()
                date_max = df['Date'].max()
                span_days = None
                try:
                    span_days = int((date_max - date_min).days)
                except Exception:
                    span_days = None

                data_coverage = {
                    'start': date_min.isoformat() if hasattr(date_min, 'isoformat') else str(date_min),
                    'end': date_max.isoformat() if hasattr(date_max, 'isoformat') else str(date_max),
                    'span_days': span_days,
                    'rows': int(len(df))
                }
            
            # ç‰¹å¾é‡è¦æ€§
            print(f"\nğŸ” ç‰¹å¾é‡è¦æ€§:")
            importances = None
            
            if hasattr(self.model, 'feature_importances_'):
                importances = self.model.feature_importances_
            elif isinstance(self.model, VotingRegressor):
                # å°è¯•è®¡ç®—å¹³å‡ç‰¹å¾é‡è¦æ€§
                try:
                    all_importances = [est.feature_importances_ for est in self.model.estimators_]
                    importances = np.mean(all_importances, axis=0)
                except Exception:
                     pass
            
            if importances is not None:
                feature_importance = pd.DataFrame({
                    'Feature': self.feature_columns,
                    'Importance': importances
                }).sort_values('Importance', ascending=False)
                
                for _, row in feature_importance.iterrows():
                    print(f"      - {row['Feature']}: {row['Importance']:.4f}")
            else:
                print("      (å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§è¾“å‡º)")
                feature_importance = pd.DataFrame(columns=['Feature', 'Importance'])
            
            # ä¿å­˜æ¨¡å‹åˆ° Firebase Storage
            print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° Firebase Storage: {self.firebase_model_path}")
            temp_model_path = None
            try:
                # Step A: åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='wb', suffix='.joblib', delete=False) as tmp_file:
                    temp_model_path = tmp_file.name
                    print(f"   - ä¸´æ—¶æ–‡ä»¶: {temp_model_path}")
                
                # Step B: ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶
                joblib.dump(self.model, temp_model_path)
                print(f"   âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶")
                
                # Step B-2: ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°æŒä¹…åŒ–è·¯å¾„ (ç”¨äºå¼€å‘ç¯å¢ƒè°ƒè¯•)
                # ä»…åœ¨é GAE ç¯å¢ƒä¸‹æ‰§è¡Œï¼Œé¿å… Read-only file system é”™è¯¯
                if not self._is_gae_environment():
                    try:
                        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
                        self.local_model_path.parent.mkdir(parents=True, exist_ok=True)
                        joblib.dump(self.model, self.local_model_path)
                        print(f"   âœ“ æ¨¡å‹å·²å¤‡ä»½åˆ°æœ¬åœ°è·¯å¾„: {self.local_model_path}")
                    except Exception as local_e:
                        print(f"   âš ï¸  æ— æ³•ä¿å­˜æœ¬åœ°æ¨¡å‹å‰¯æœ¬: {str(local_e)}")
                else:
                    print(f"   â„¹ï¸  GAE ç¯å¢ƒï¼šè·³è¿‡æœ¬åœ°æ¨¡å‹å¤‡ä»½")

                
                # Step C: ä¸Šä¼ åˆ° Firebase Storage
                with open(temp_model_path, 'rb') as f:
                    self.storage_service.upload_file(
                        file_data=f,
                        destination_path=self.firebase_model_path,
                        content_type='application/octet-stream'
                    )
                print(f"   âœ“ æ¨¡å‹å·²ä¸Šä¼ åˆ° Firebase Storage")
                
            except Exception as e:
                print(f"   âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
                raise
            finally:
                # Step D: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_model_path and os.path.exists(temp_model_path):
                    try:
                        os.remove(temp_model_path)
                        print(f"   ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶")
                    except Exception as e:
                        print(f"   âš ï¸  æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            print("\n" + "="*80)
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
            print("="*80 + "\n")
            
            # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®åˆ° Firebase Storage (å…¨å±€å…ƒæ•°æ®)
            # è·å–æ¨¡å‹ç±»å‹åç§°
            model_type_name = self._get_model_type_name()
            
            try:
                metadata = {
                    'model_type': model_type_name,
                    'model_version': datetime.now().strftime('%Y%m%d_%H%M%S'),
                    'trained_at': datetime.now().isoformat(),
                    'metrics': {
                        'train_mae': float(train_mae),
                        'train_rmse': float(train_rmse),
                        'test_mae': float(test_mae),
                        'test_rmse': float(test_rmse),
                        'r2_score': float(test_r2),
                        'mape': float(test_mape / 100),  # å­˜å‚¨ä¸ºå°æ•°å½¢å¼, 5% -> 0.05
                    },
                    'training_samples': len(df),
                    'data_source': 'CAISO Real-Time Stream',
                    'feature_importance': feature_importance.to_dict('records'),
                    'model_path': self.firebase_model_path,
                    'status': 'active',
                    # æ•°æ®è¦†ç›–ä¿¡æ¯
                    'data_coverage': data_coverage,
                    # ã€æ–°å¢ã€‘å•ä½æ ‡æ³¨ä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼Œä¸æ”¹å˜æ•°å€¼ï¼‰
                    'units': {
                        'target_variable': self.target_column,
                        'target_unit': 'kW',  # Site_Load çš„å•ä½
                        'load_unit_multiplier': 1.0,  # 1.0 è¡¨ç¤ºåŸå§‹å€¼ï¼Œ1000 è¡¨ç¤º MW->kW
                        'price_unit': 'CNY/kWh',  # ç”µä»·å•ä½
                        'temperature_unit': 'Â°C',  # æ¸©åº¦å•ä½
                        'note': 'CAISO åŸå§‹æ•°æ®ä¸º MWï¼Œå·²è½¬æ¢ä¸º kW å­˜å‚¨'
                    },
                    # ç‰¹å¾å·¥ç¨‹ä¿¡æ¯
                    'feature_engineering': {
                        'total_features': len(self.feature_columns),
                        'base_features': len(self.base_feature_columns),
                        'enhanced_features': len(enhanced_features_used) if use_enhanced_features else 0,
                        'enhanced_features_list': enhanced_features_used if use_enhanced_features else [],
                        'use_enhanced': use_enhanced_features
                    },
                    # éªŒè¯æ‘˜è¦ (äº¤å‰éªŒè¯ / ç•™å‡ºéªŒè¯)
                    'validation_summary': validation_summary,
                    # ã€æ–°å¢ã€‘è®­ç»ƒé…ç½®è®°å½•ï¼ˆä¾¿äºå¤ç°ï¼‰
                    'training_config': {
                        'test_size': test_size,
                        'random_state': random_state,
                        'use_time_series_cv': use_time_series_cv,
                        'cv_folds': cv_folds if use_time_series_cv else None,
                        'cv_folds': cv_folds if use_time_series_cv else None,
                        'time_series_split': use_time_series_cv,  # æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†æ—¶é—´åºåˆ—æ‹†åˆ†
                        'use_log_transform': use_log_transform,   # è®°å½•æ˜¯å¦ä½¿ç”¨äº† Log å˜æ¢
                        'remove_outliers': remove_outliers,       # è®°å½•æ˜¯å¦ä½¿ç”¨äº†å¼‚å¸¸å€¼è¿‡æ»¤
                        'hyperparameter_tuning': tune_hyperparameters # è®°å½•æ˜¯å¦å¯ç”¨äº†è¶…å‚æ•°æœç´¢
                    }
                }
                
                # æ·»åŠ è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ä¿¡æ¯
                if auto_select_model and model_type == 'auto':
                    metadata['auto_selection'] = {
                        'enabled': True,
                        'candidates_evaluated': selection_info.get('candidates_evaluated', []),
                        'winner': selection_info.get('winner', 'unknown'),
                        'improvement_over_baseline': selection_info.get('improvement', 'N/A'),
                        'all_scores': selection_info.get('all_scores', {}),
                        # æ–°å¢ï¼šäº¤å‰éªŒè¯ä¿¡æ¯
                        'validation_method': selection_info.get('validation_method', 'HoldOut'),
                        'cv_folds': selection_info.get('cv_folds'),
                        'cv_details': selection_info.get('cv_details')
                    }
                    metadata['hyperparameters'] = hyperparameters
                
                self._save_model_metadata(metadata)
            except Exception as e:
                print(f"   âš ï¸  ä¿å­˜æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            
            # è¿”å›è¯„ä¼°æŒ‡æ ‡
            return {
                'train_mae': train_mae,
                'train_rmse': train_rmse,
                'test_mae': test_mae,
                'test_rmse': test_rmse,
                'r2_score': test_r2,  # æ–°å¢
                'mape': test_mape / 100,  # æ–°å¢ (å°æ•°å½¢å¼)
                'feature_importance': feature_importance.to_dict('records'),
                'model_type': model_type_name,
                'hyperparameters': hyperparameters if auto_select_model else {'n_estimators': n_estimators},
                'auto_selection': selection_info if auto_select_model and model_type == 'auto' else None,
                # æ–°å¢ï¼šç‰¹å¾å·¥ç¨‹å’Œäº¤å‰éªŒè¯ä¿¡æ¯
                'feature_engineering': {
                    'total_features': len(self.feature_columns),
                    'enhanced_features_used': enhanced_features_used if use_enhanced_features else []
                },
                'validation': {
                    'method': 'TimeSeriesSplit' if use_time_series_cv else 'HoldOut',
                    'cv_folds': cv_folds if use_time_series_cv else None,
                    'summary': validation_summary
                },
                'data_coverage': data_coverage
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_data_path and os.path.exists(temp_data_path):
                try:
                    os.remove(temp_data_path)
                    print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶")
                except Exception as e:
                    print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def load_model(self) -> bool:
        """
        åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆä¼˜å…ˆä» Firebase Storageï¼‰
        åŒæ—¶åŠ è½½æ¨¡å‹å…ƒæ•°æ®ä»¥æ¢å¤æ­£ç¡®çš„ç‰¹å¾åˆ—è¡¨
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
            
        Raises:
            FileNotFoundError: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨
            Exception: åŠ è½½æ¨¡å‹æ—¶å‡ºé”™
        """
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹...")
        temp_model_path = None
        
        try:
            # Step A: æ£€æŸ¥ Firebase Storage ä¸­æ˜¯å¦å­˜åœ¨æ¨¡å‹
            print(f"   - æ£€æŸ¥ Firebase Storage: {self.firebase_model_path}")
            
            if self.storage_service.file_exists(self.firebase_model_path):
                print(f"   âœ“ Firebase Storage ä¸­å­˜åœ¨æ¨¡å‹")
                
                # Step B: ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
                temp_model_path = self.storage_service.download_to_temp(self.firebase_model_path)
                
                if temp_model_path is None:
                    raise Exception("ä» Firebase Storage ä¸‹è½½æ¨¡å‹å¤±è´¥")
                
                print(f"   âœ“ æ¨¡å‹å·²ä¸‹è½½åˆ°: {temp_model_path}")
                
                # Step C: ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ¨¡å‹
                self.model = joblib.load(temp_model_path)
                print(f"   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (æ¥æº: Firebase Storage)")
                
                # Step D: åŠ è½½æ¨¡å‹å…ƒæ•°æ®ä»¥æ¢å¤ç‰¹å¾åˆ—è¡¨
                self._load_feature_columns_from_metadata()
                
                return True
            
            else:
                # Step E: Firebase ä¸­æ²¡æœ‰æ¨¡å‹ï¼Œå°è¯•åŠ è½½æœ¬åœ°å…œåº•æ¨¡å‹
                print(f"   âš ï¸  Firebase Storage ä¸­æ— æ¨¡å‹ï¼Œå°è¯•åŠ è½½æœ¬åœ°å…œåº•æ¨¡å‹")
                
                if not self.local_model_path.exists():
                    raise FileNotFoundError(
                        f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨:\n"
                        f"  - Firebase Storage: {self.firebase_model_path} (ä¸å­˜åœ¨)\n"
                        f"  - æœ¬åœ°è·¯å¾„: {self.local_model_path} (ä¸å­˜åœ¨)\n"
                        f"è¯·å…ˆè°ƒç”¨ train_model() è®­ç»ƒæ¨¡å‹ã€‚"
                    )
                
                self.model = joblib.load(self.local_model_path)
                print(f"   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (æ¥æº: æœ¬åœ°å…œåº•æ–‡ä»¶)")
                
                # åŒæ ·å°è¯•åŠ è½½å…ƒæ•°æ®
                self._load_feature_columns_from_metadata()
                
                return True
        
        except Exception as e:
            raise Exception(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_model_path and os.path.exists(temp_model_path):
                try:
                    os.remove(temp_model_path)
                    print(f"   ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶")
                except Exception as e:
                    print(f"   âš ï¸  æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _load_history_context(self, end_time: datetime, window_size: int = 200) -> pd.DataFrame:
        """
        åŠ è½½ç”¨äºç‰¹å¾æ„å»ºçš„å†å²æ•°æ®ä¸Šä¸‹æ–‡
        
        Args:
            end_time: æˆªæ­¢æ—¶é—´ï¼ˆä¸åŒ…å«ï¼‰
            window_size: éœ€è¦åŠ è½½çš„å†å²çª—å£å¤§å°ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            åŒ…å« Site_Load, Temperature ç­‰åˆ—çš„å†å² DataFrame
        """
        print(f"ğŸ“– åŠ è½½å†å²ä¸Šä¸‹æ–‡ (æˆªæ­¢ {end_time})...")
        
        # å°è¯•åŠ è½½å…¨é‡æ•°æ®æ–‡ä»¶
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™åº”è¯¥ä¼˜åŒ–ä¸ºåªä»æ•°æ®åº“/Storageè¯»å–éƒ¨åˆ†æ•°æ®
        # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿™é‡Œå¤ç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶
        data_path = None
        
        # å°è¯•ä»æœ¬åœ°æˆ–ä¸´æ—¶ç›®å½•æŸ¥æ‰¾
        possible_paths = [
            self.back_dir.parent / 'data' / 'processed' / 'cleaned_energy_data_all.csv', # å¼€å‘ç¯å¢ƒ
            Path('/tmp/cleaned_energy_data_all.csv'), # ä¸´æ—¶ç›®å½•
            self.back_dir / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
        ]
        
        for path in possible_paths:
            if path.exists():
                data_path = path
                break
                
        if data_path is None:
            # å°è¯•ä» Storage ä¸‹è½½
            print("   ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•ä» Firebase Storage ä¸‹è½½...")
            try:
                data_path = self.storage_service.download_to_temp('data/processed/cleaned_energy_data_all.csv')
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•ä¸‹è½½å†å²æ•°æ®: {e}")
                
        if data_path:
            df = pd.read_csv(data_path, parse_dates=['Date'])
            # ç­›é€‰æˆªæ­¢æ—¶é—´ä¹‹å‰çš„æ•°æ®
            history = df[df['Date'] < end_time].tail(window_size).copy()
            if len(history) < 168:
                print(f"   âš ï¸  å†å²æ•°æ®ä¸è¶³ 168 å°æ—¶ (å®é™…: {len(history)}), ç‰¹å¾å¯èƒ½ä¸å‡†ç¡®")
            return history
        else:
            print("   âš ï¸  æ— æ³•åŠ è½½å†å²ä¸Šä¸‹æ–‡ï¼Œå°†ä½¿ç”¨å…¨ 0 å¡«å…… (ä»…ç”¨äºæµ‹è¯•/å†·å¯åŠ¨)")
            # åˆ›å»ºè™šæ‹Ÿå†å²æ•°æ®
            dates = [end_time - timedelta(hours=i) for i in range(window_size, 0, -1)]
            return pd.DataFrame({
                'Date': dates,
                'Site_Load': [0.0] * window_size,
                'Temperature': [25.0] * window_size,
                'Hour': [d.hour for d in dates],
                'DayOfWeek': [d.weekday() for d in dates]
            })

    def predict_next_24h(
        self,
        start_time: Union[str, datetime],
        temp_forecast_list: Optional[List[float]] = None,
        temp_adjust_delta: float = 0.0
    ) -> List[Dict[str, Union[datetime, float]]]:
        """
        é¢„æµ‹æœªæ¥24å°æ—¶çš„èƒ½æºè´Ÿè½½ (é€’å½’é¢„æµ‹æ¨¡å¼)
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            temp_forecast_list: æ¸©åº¦é¢„æµ‹åˆ—è¡¨
            temp_adjust_delta: æ¸©åº¦è°ƒæ•´å€¼ (ç”¨äº What-If åˆ†æ)
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")
        
        if isinstance(start_time, str):
            start_time = pd.to_datetime(start_time)
        
        # éªŒè¯æ¸©åº¦é¢„æµ‹åˆ—è¡¨é•¿åº¦
        if temp_forecast_list is not None and len(temp_forecast_list) != 24:
            raise ValueError(f"temp_forecast_list é•¿åº¦å¿…é¡»ä¸º 24ï¼Œå½“å‰ä¸º {len(temp_forecast_list)}")
            
        print(f"\nğŸ”® é€’å½’é¢„æµ‹æœªæ¥24å°æ—¶è´Ÿè½½ (ä» {start_time} å¼€å§‹)...")
        
        # 1. åŠ è½½å†å²ä¸Šä¸‹æ–‡ (ç”¨äºæ„å»º Lag/Rolling ç‰¹å¾)
        # æˆ‘ä»¬è‡³å°‘éœ€è¦è¿‡å» 168 å°æ—¶çš„æ•°æ®
        history_df = self._load_history_context(start_time, window_size=200)
        
        # è½¬æ¢ä¸º list ä»¥ä¾¿é«˜æ•ˆ append
        history_loads = history_df['Site_Load'].tolist()
        history_temps = history_df['Temperature'].tolist() # å†å²æ¸©åº¦
        
        # å¦‚æœæœªæä¾›æ¸©åº¦é¢„æµ‹ï¼Œä½¿ç”¨æŒä¹…æ€§é¢„æµ‹ (æ˜¨å¤©çš„æ¸©åº¦)
        if temp_forecast_list is None:
            if len(history_temps) >= 24:
                # ä½¿ç”¨è¿‡å» 24 å°æ—¶çš„æ•°æ®ä½œä¸ºåŸºå‡† (Persistence Forecast)
                print("   â„¹ï¸  æœªæä¾›æ¸©åº¦é¢„æµ‹ï¼Œä½¿ç”¨è¿‡å»24å°æ—¶æ¸©åº¦ä½œä¸ºåŸºå‡† (Persistence Forecast)")
                temp_forecast_list = history_temps[-24:]
            else:
                # å†å²æ•°æ®ä¸è¶³ï¼Œå›é€€åˆ°é»˜è®¤å€¼
                print("   âš ï¸  å†å²æ¸©åº¦ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤ 25.0Â°C")
                temp_forecast_list = [25.0] * 24
        
        # åº”ç”¨æ¸©åº¦è°ƒæ•´ (What-If Analysis)
        if temp_adjust_delta != 0.0:
            print(f"   ğŸŒ¡ï¸  åº”ç”¨æ¸©åº¦è°ƒæ•´: {temp_adjust_delta:+.1f}Â°C")
            temp_forecast_list = [t + temp_adjust_delta for t in temp_forecast_list]
        
        predictions = []
        prediction_results = []
        
        # 2. é€’å½’é¢„æµ‹å¾ªç¯
        current_time = start_time
        
        for i in range(24):
            # A. ç‰¹å¾æ„å»º
            hour = current_time.hour
            day_of_week = current_time.weekday()
            temperature = temp_forecast_list[i]
            price = self._get_price(hour)
            
            # æ„å»ºé«˜çº§ç‰¹å¾
            # æ³¨æ„ï¼šhistory_loads çš„æœ€åä¸€ä¸ªå…ƒç´ æ˜¯ t-1 æ—¶åˆ»çš„è´Ÿè½½
            
            # Lag Features
            lag_1h = history_loads[-1] if len(history_loads) >= 1 else 0
            lag_24h = history_loads[-24] if len(history_loads) >= 24 else 0
            lag_168h = history_loads[-168] if len(history_loads) >= 168 else 0
            
            # Rolling Features
            # å–æœ€è¿‘ N ä¸ªç‚¹è®¡ç®—å‡å€¼/æ ‡å‡†å·®
            # ã€ä¿®å¤ã€‘ä½¿ç”¨ ddof=1 ä¸è®­ç»ƒæ—¶ pandas.rolling().std() ä¿æŒä¸€è‡´
            roll_6h_mean = np.mean(history_loads[-6:]) if len(history_loads) >= 6 else lag_1h
            roll_6h_std = np.std(history_loads[-6:], ddof=1) if len(history_loads) >= 6 else 0
            roll_24h_mean = np.mean(history_loads[-24:]) if len(history_loads) >= 24 else lag_1h
            
            # Interaction Features (åŸºç¡€)
            temp_x_hour = temperature * hour
            lag24_x_dow = lag_24h * day_of_week
            
            # ç»„è£…åŸºç¡€ç‰¹å¾å‘é‡
            feature_dict = {
                'Hour': hour,
                'DayOfWeek': day_of_week,
                'Temperature': temperature,
                'Price': price,
                'Lag_1h': lag_1h,
                'Lag_24h': lag_24h,
                'Lag_168h': lag_168h,
                'Rolling_Mean_6h': roll_6h_mean,
                'Rolling_Std_6h': roll_6h_std,
                'Rolling_Mean_24h': roll_24h_mean,
                'Temp_x_Hour': temp_x_hour,
                'Lag24_x_DayOfWeek': lag24_x_dow
            }
            
            # æ·»åŠ å¢å¼ºç‰¹å¾ï¼ˆå¦‚æœæ¨¡å‹éœ€è¦ï¼‰
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰¹å¾
            if len(self.feature_columns) > 12:
                # æ—¶é—´ç‰¹å¾
                month = current_time.month
                day_of_month = current_time.day
                week_of_year = current_time.isocalendar()[1]
                
                # å­£èŠ‚ (åŒ—åŠçƒ)
                if month in [3, 4, 5]:
                    season = 0  # æ˜¥
                elif month in [6, 7, 8]:
                    season = 1  # å¤
                elif month in [9, 10, 11]:
                    season = 2  # ç§‹
                else:
                    season = 3  # å†¬
                
                # æ˜¯å¦å‘¨æœ«
                is_weekend = 1 if day_of_week >= 5 else 0
                
                # æ˜¯å¦èŠ‚å‡æ—¥ï¼ˆç¾å›½åŠ å·ï¼‰
                # ä¼˜åŒ–: é¿å…åœ¨å¾ªç¯ä¸­é‡å¤åˆå§‹åŒ–
                if not hasattr(self, '_holidays_cache'):
                    try:
                        import holidays
                        # ä½¿ç”¨ subdiv æ›¿ä»£ deprecated çš„ state å‚æ•°
                        self._holidays_cache = holidays.US(subdiv='CA')
                    except ImportError:
                        self._holidays_cache = None
                    except Exception:
                        # å…¼å®¹æ—§ç‰ˆæœ¬ holidays (å¦‚ä¸æ”¯æŒ subdiv)
                        try:
                            import holidays
                            self._holidays_cache = holidays.US(state='CA')
                        except:
                            self._holidays_cache = None

                if self._holidays_cache:
                    is_holiday = 1 if current_time.date() in self._holidays_cache else 0
                else:
                    is_holiday = is_weekend  # ç®€åŒ–ï¼šå‘¨æœ«è§†ä¸ºå‡æ—¥
                
                # å¢å¼ºäº¤äº’ç‰¹å¾
                temp_x_season = temperature * season
                lag24_x_is_weekend = lag_24h * is_weekend
                hour_x_is_holiday = hour * is_holiday
                
                # å‘¨æœŸç¼–ç 
                month_sin = np.sin(2 * np.pi * month / 12)
                month_cos = np.cos(2 * np.pi * month / 12)
                hour_sin = np.sin(2 * np.pi * hour / 24)
                hour_cos = np.cos(2 * np.pi * hour / 24)
                
                # æ·»åŠ å¢å¼ºç‰¹å¾
                feature_dict.update({
                    'Month': month,
                    'Season': season,
                    'IsWeekend': is_weekend,
                    'IsHoliday': is_holiday,
                    'DayOfMonth': day_of_month,
                    'WeekOfYear': week_of_year,
                    'Temp_x_Season': temp_x_season,
                    'Lag24_x_IsWeekend': lag24_x_is_weekend,
                    'Hour_x_IsHoliday': hour_x_is_holiday,
                    'Month_Sin': month_sin,
                    'Month_Cos': month_cos,
                    'Hour_Sin': hour_sin,
                    'Hour_Cos': hour_cos
                })
            
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸æ¨¡å‹ä¸€è‡´
            features = pd.DataFrame([{col: feature_dict[col] for col in self.feature_columns}])
            
            # B. å•æ­¥æ¨ç†
            pred_log = float(self.model.predict(features)[0])
            
            # å¦‚æœæ¨¡å‹ä½¿ç”¨äº† Log å˜æ¢ï¼Œéœ€è¦è¿˜åŸ
            if hasattr(self, 'use_log_transform') and self.use_log_transform:
                pred_load = float(np.expm1(pred_log))
            else:
                pred_load = pred_log
            
            # ã€å®‰å…¨æ£€æŸ¥ã€‘å¼ºåˆ¶çº¦æŸé¢„æµ‹å€¼ä¸ºéè´Ÿ (ç‰©ç†å¸¸è¯†)
            pred_load = max(0.0, pred_load)
            
            # C. æ›´æ–°å†å²åºåˆ— (é€’å½’å…³é”®)
            # å°†é¢„æµ‹å€¼ä½œä¸º"çœŸå®å€¼"åŠ å…¥å†å²ï¼Œç”¨äºä¸‹ä¸€æ­¥é¢„æµ‹
            history_loads.append(pred_load)
            predictions.append(pred_load)
            
            # D. è®°å½•ç»“æœ
            prediction_results.append({
                'datetime': current_time,
                'predicted_load': pred_load,
                'temperature': temperature,
                'price': price,
                'hour': hour,
                'day_of_week': day_of_week
            })
            
            # æ—¶é—´æ­¥è¿›
            current_time += timedelta(hours=1)
            
        print(f"   âœ“ é€’å½’é¢„æµ‹å®Œæˆ")
        return prediction_results

    def predict_single(self, *args, **kwargs):
        """
        å•ç‚¹é¢„æµ‹å·²è¢«é€’å½’é¢„æµ‹å–ä»£ï¼Œä¸”ä¸ä»…ä¾èµ–ç®€å•è¾“å…¥ã€‚
        ä¸ºä¿æŒæ¥å£å…¼å®¹æŠ›å‡ºå¼‚å¸¸æˆ–ä»…åšç®€å•å¤„ç†ã€‚
        """
        raise NotImplementedError("å•ç‚¹é¢„æµ‹ (predict_single) å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ predict_next_24h è¿›è¡Œåºåˆ—é¢„æµ‹ã€‚")
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        è·å–æ¨¡å‹çš„å…¨å±€ç‰¹å¾é‡è¦æ€§
        
        ä½¿ç”¨éšæœºæ£®æ—å†…ç½®çš„ feature_importances_ å±æ€§ï¼Œ
        åæ˜ å„ç‰¹å¾åœ¨æ•´ä½“é¢„æµ‹ä¸­çš„å¹³å‡è´¡çŒ®
        
        Returns:
            ç‰¹å¾ååˆ°é‡è¦æ€§åˆ†æ•°çš„æ˜ å°„å­—å…¸
            
        Raises:
            ValueError: æ¨¡å‹æœªåŠ è½½
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")
        
        importances = None
        if hasattr(self.model, 'feature_importances_'):
             importances = self.model.feature_importances_
        elif isinstance(self.model, VotingRegressor):
             try:
                 all_importances = [est.feature_importances_ for est in self.model.estimators_]
                 importances = np.mean(all_importances, axis=0)
             except:
                 pass
                 
        if importances is None:
             # å¦‚æœæ— æ³•è·å–ï¼Œè¿”å›ç©ºå­—å…¸æˆ–é»˜è®¤å‡åŒ€åˆ†å¸ƒ
             return {col: 0.0 for col in self.feature_columns}
             
        # ã€ä¿®å¤ã€‘å½’ä¸€åŒ–ç‰¹å¾é‡è¦æ€§ï¼Œç¡®ä¿æ€»å’Œä¸º 1.0
        # XGBoost/LightGBM åœ¨ VotingRegressor ä¸­å¯èƒ½è¿”å›æœªå½’ä¸€åŒ–çš„å€¼ (å¦‚ gain/cover)
        total_importance = np.sum(importances)
        if total_importance > 0:
            importances = importances / total_importance
             
        importance_dict = dict(zip(
            self.feature_columns,
            [float(v) for v in importances]
        ))
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        sorted_importance = dict(sorted(
            importance_dict.items(),
            key=lambda x: x[1],
            reverse=True
        ))
        
        return sorted_importance
    
    def explain_prediction(
        self,
        hour: int,
        day_of_week: int,
        temperature: float,
        price: float = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ SHAP è§£é‡Šå•æ¬¡é¢„æµ‹
        
        ä¸ºç»™å®šè¾“å…¥æä¾›è¯¦ç»†çš„ç‰¹å¾è´¡çŒ®åˆ†æï¼Œ
        å±•ç¤ºæ¯ä¸ªç‰¹å¾å¦‚ä½•å½±å“æœ€ç»ˆé¢„æµ‹å€¼
        
        Args:
            hour: å°æ—¶ (0-23)
            day_of_week: æ˜ŸæœŸå‡  (0-6)
            temperature: æ¸©åº¦ (Â°C)
            price: ç”µä»·ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ ¹æ®hourè®¡ç®—
            
        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
            - base_value: æ¨¡å‹åŸºå‡†é¢„æµ‹å€¼ï¼ˆè®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼‰
            - predicted_value: å®é™…é¢„æµ‹å€¼
            - feature_contributions: å„ç‰¹å¾çš„è´¡çŒ®å€¼å­—å…¸
            - interpretation: äººç±»å¯è¯»çš„è§£é‡Šæ–‡å­—
        """
        try:
            import shap
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")

            # æ„å»ºç‰¹å¾ DataFrame
            # ã€ä¿®å¤ã€‘å¦‚æœ price ä¸º Noneï¼Œä½¿ç”¨ç»Ÿä¸€çš„ _get_price æ–¹æ³•è®¡ç®—ï¼ˆä¸æ¨ç†ä¸€è‡´ï¼‰
            if price is None:
                price = self._get_price(hour)
            
            # å°è¯•ä»å†å²æ•°æ®è·å–æ»åç‰¹å¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
            # é»˜è®¤å€¼åŸºäºå…¸å‹çš„è´Ÿè½½æ¨¡å¼
            default_load = 150.0  # å…¸å‹å¹³å‡è´Ÿè½½ (kW)
            
            # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾ DataFrameï¼ˆåŒ…å«æ‰€æœ‰12ä¸ªç‰¹å¾ï¼‰
            features = pd.DataFrame({
                'Hour': [hour],
                'DayOfWeek': [day_of_week],
                'Temperature': [temperature],
                'Price': [price],
                'Lag_1h': [default_load],  # 1å°æ—¶å‰çš„è´Ÿè½½
                'Lag_24h': [default_load],  # 24å°æ—¶å‰çš„è´Ÿè½½
                'Lag_168h': [default_load],  # 168å°æ—¶(ä¸€å‘¨)å‰çš„è´Ÿè½½
                'Rolling_Mean_6h': [default_load],  # 6å°æ—¶æ»šåŠ¨å¹³å‡
                'Rolling_Std_6h': [default_load * 0.1],  # 6å°æ—¶æ»šåŠ¨æ ‡å‡†å·®
                'Rolling_Mean_24h': [default_load],  # 24å°æ—¶æ»šåŠ¨å¹³å‡
                'Temp_x_Hour': [temperature * hour],  # æ¸©åº¦ä¸å°æ—¶çš„äº¤äº’ç‰¹å¾
                'Lag24_x_DayOfWeek': [default_load * day_of_week]  # 24å°æ—¶æ»åä¸æ˜ŸæœŸçš„äº¤äº’
            })
            
            # ç¡®ä¿ç‰¹å¾åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
            features = features[self.feature_columns]
            
            # ä½¿ç”¨ TreeExplainer è§£é‡Šéšæœºæ£®æ—æ¨¡å‹
            # åªæœ‰å½“ explainer å°šæœªåˆå§‹åŒ–æ—¶æ‰åˆ›å»ºï¼Œé¿å…é‡å¤è®¡ç®—
            if not hasattr(self, '_shap_explainer') or self._shap_explainer is None:
                self._shap_explainer = shap.TreeExplainer(self.model)
                
            # è®¡ç®— SHAP å€¼
            shap_values = self._shap_explainer.shap_values(features)
            
            # è·å–æœŸæœ›å€¼ (base value)
            # å¯¹äºå›å½’æ¨¡å‹ï¼Œexpected_value åº”è¯¥æ˜¯ä¸€ä¸ªæ ‡é‡
            base_value = self._shap_explainer.expected_value
            if isinstance(base_value, np.ndarray):
                base_value = base_value[0]
                
            # è·å–å½“å‰é¢„æµ‹çš„ SHAP å€¼
            # shap_values å¯¹äºå›å½’å¯èƒ½æ˜¯ (n_samples, n_features)
            # æˆ‘ä»¬åªéœ€è¦ç¬¬ä¸€ä¸ªæ ·æœ¬
            current_shap_values = shap_values[0]
            
            # é¢„æµ‹å€¼ = base_value + sum(shap_values)
            predicted_value = base_value + np.sum(current_shap_values)
            
            # æ„å»ºç‰¹å¾è´¡çŒ®åˆ—è¡¨
            contributions = []
            for i, col in enumerate(self.feature_columns):
                contributions.append({
                    'feature': col,
                    'value': float(features.iloc[0][col]),
                    'contribution': float(current_shap_values[i])
                })
            
            # æŒ‰è´¡çŒ®ç»å¯¹å€¼æ’åº
            contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)
            
            # ç”Ÿæˆäººç±»å¯è¯»çš„è§£é‡Šæ–‡å­—
            top_feature = contributions[0]
            direction = "å¢åŠ " if top_feature['contribution'] > 0 else "å‡å°‘"
            interpretation = (
                f"{top_feature['feature']} æ˜¯å½±å“æœ€å¤§çš„å› ç´ ï¼Œ"
                f"å®ƒä½¿å¾—é¢„æµ‹è´Ÿè½½{direction}äº† {abs(top_feature['contribution']):.1f} kWã€‚"
            )

            return {
                'base_value': float(base_value),
                'predicted_value': float(predicted_value),
                'feature_contributions': contributions,
                'interpretation': interpretation
            }
            
        except Exception as e:
            print(f"è§£é‡Šé¢„æµ‹å¤±è´¥: {str(e)}")
            return None

    def evaluate_recent_performance(self, hours: int = 24) -> Dict[str, Union[float, str]]:
        """
        è¯„ä¼°æ¨¡å‹åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´çš„è¡¨ç° (åœ¨çº¿ç›‘æ§)
        é€šè¿‡å›æµ‹æœ€è¿‘çš„çœŸå®æ•°æ®æ¥è®¡ç®—æŒ‡æ ‡
        
        Args:
            hours: å›æµ‹çš„å°æ—¶æ•° (é»˜è®¤ 24)
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ (mape, r2, last_update_time)
        """
        print(f"\nğŸ” å¼€å§‹åœ¨çº¿æ¨¡å‹è¯„ä¼° (æœ€è¿‘ {hours} å°æ—¶)...")
        
        try:
            # 1. åŠ¨æ€ä¸‹è½½æœ€æ–°æ•°æ® (ä» Firestore/Storage æŒä¹…åŒ–çš„ CSV)
            from services.storage_service import StorageService
            storage_service = StorageService()
            
            # å°è¯•ä¸‹è½½æœ€æ–°çš„ cleaned_energy_data_all.csv
            data_path = storage_service.download_to_temp('data/processed/cleaned_energy_data_all.csv')
            
            if not data_path:
                print("   âš ï¸ æ— æ³•ä¸‹è½½æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°")
                return {'status': 'no_data'}
                
            # 2. è¯»å–æ•°æ®
            df = pd.read_csv(data_path, parse_dates=['Date'])
            
            # 3. åŸºäºæ—¶é—´æˆªå–æœ€è¿‘ N å°æ—¶çš„æ•°æ® (é¿å…æ•°æ®ä¸­æ–­å¯¼è‡´ tail(N) è·¨åº¦è¿‡å¤§)
            last_time = df['Date'].max()
            start_time = last_time - timedelta(hours=hours)
            
            # ç­›é€‰æ—¶é—´çª—å£å†…çš„æ•°æ®
            recent_df = df[df['Date'] > start_time].copy()
            
            # æ£€æŸ¥æ ·æœ¬é‡
            # ç†è®ºä¸Šåº”è¯¥æœ‰ hours ä¸ªæ ·æœ¬ï¼Œå…è®¸æœ‰ä¸€ç‚¹ç¼ºå¤± (e.g. > 50%)
            min_samples = int(hours * 0.5) 
            if len(recent_df) < min_samples:
                print(f"   âš ï¸ æœ€è¿‘ {hours} å°æ—¶å†…æ•°æ®æ ·æœ¬ä¸è¶³ ({len(recent_df)} < {min_samples})ï¼Œè·³è¿‡è¯„ä¼°")
                return {
                    'status': 'insufficient_data',
                    'message': f'Insufficient data in last {hours}h: found {len(recent_df)} samples'
                }
                
            # 4. å‡†å¤‡ç‰¹å¾å’ŒçœŸå®å€¼
            # ã€ä¿®å¤ã€‘æ£€æŸ¥å¹¶è‡ªåŠ¨è¡¥ç®—ç¼ºå¤±ç‰¹å¾
            missing_features = [col for col in self.feature_columns if col not in recent_df.columns]
            if missing_features:
                print(f"   âš ï¸ æ£€æµ‹åˆ°ç¼ºå¤±ç‰¹å¾: {missing_features}")
                print(f"   ğŸ”§ å°è¯•è‡ªåŠ¨è¡¥ç®—...")
                
                try:
                    from services.data_processor import EnergyDataProcessor
                    processor = EnergyDataProcessor()
                    
                    # ç¡®ä¿ Date æ˜¯ datetime ç±»å‹
                    if recent_df['Date'].dtype == 'object':
                        recent_df['Date'] = pd.to_datetime(recent_df['Date'])
                    
                    # è¡¥ç®—åŸºç¡€æ—¶é—´ç‰¹å¾
                    if 'Hour' not in recent_df.columns:
                        recent_df['Hour'] = recent_df['Date'].dt.hour
                    if 'DayOfWeek' not in recent_df.columns:
                        recent_df['DayOfWeek'] = recent_df['Date'].dt.dayofweek
                    if 'Price' not in recent_df.columns:
                        recent_df = processor.add_price_feature(recent_df)
                    
                    # è¡¥ç®—å¢å¼ºæ—¶é—´ç‰¹å¾
                    enhanced_time_cols = ['Month', 'Season', 'IsWeekend', 'IsHoliday']
                    if any(c in missing_features for c in enhanced_time_cols):
                        recent_df = processor.add_enhanced_time_features(recent_df)
                    
                    # è¡¥ç®— Lag/Rolling ç‰¹å¾
                    lag_cols = ['Lag_1h', 'Lag_24h', 'Lag_168h', 'Rolling_Mean_6h', 'Rolling_Std_6h']
                    if any(c in missing_features for c in lag_cols):
                        recent_df = processor.add_advanced_features(recent_df, dropna=False, use_enhanced=True)
                    
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç¼ºå¤±
                    still_missing = [col for col in self.feature_columns if col not in recent_df.columns]
                    if still_missing:
                        print(f"   âŒ ä»æœ‰æ— æ³•è¡¥ç®—çš„ç‰¹å¾: {still_missing}")
                        return {
                            'status': 'feature_mismatch',
                            'message': f'Cannot compute features: {still_missing}'
                        }
                    print(f"   âœ“ ç‰¹å¾è¡¥ç®—å®Œæˆ")
                except Exception as fe:
                    print(f"   âŒ ç‰¹å¾è¡¥ç®—å¤±è´¥: {str(fe)}")
                    return {
                        'status': 'feature_error',
                        'message': f'Feature computation failed: {str(fe)}'
                    }
            
            # åˆ é™¤å›  Lag/Rolling äº§ç”Ÿçš„ NaN è¡Œ
            recent_df = recent_df.dropna(subset=self.feature_columns)
            if len(recent_df) < min_samples:
                print(f"   âš ï¸ åˆ é™¤ NaN åæ ·æœ¬ä¸è¶³ ({len(recent_df)} < {min_samples})")
                return {
                    'status': 'insufficient_data',
                    'message': f'Insufficient valid samples after feature computation'
                }
            
            X_recent = recent_df[self.feature_columns]
            y_true = recent_df[self.target_column].values
            
            # 5. è¿›è¡Œé¢„æµ‹
            y_pred = self.model.predict(X_recent)
            
            # å¦‚æœæ¨¡å‹ä½¿ç”¨äº† Log å˜æ¢ï¼Œéœ€è¦è¿˜åŸ
            if hasattr(self, 'use_log_transform') and self.use_log_transform:
                 y_pred = np.expm1(y_pred)
            
            # 6. è®¡ç®—æŒ‡æ ‡
            # MAPE: Mean Absolute Percentage Error
            # é¿å…åˆ†æ¯ä¸º 0
            mask = y_true != 0
            if np.sum(mask) == 0:
                print("   âš ï¸ æ‰€æœ‰çœŸå®è´Ÿè½½å‡ä¸º 0ï¼Œæ— æ³•è®¡ç®— MAPE")
                mape = 0.0
            else:
                mape = (np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100)
            
            # R2 Score
            if len(y_true) < 2:
                r2 = 0.0  # æ ·æœ¬å¤ªå°‘
            else:
                r2 = r2_score(y_true, y_pred)
            
            # 7. æ ¼å¼åŒ–ç»“æœ
            # æ³¨æ„ï¼šmape å­˜å‚¨ä¸ºå°æ•°å½¢å¼ (0.05 = 5%)ï¼Œä»¥ä¾¿ä¸å‰ç«¯ percent indicator ç›´æ¥å…¼å®¹
            metrics = {
                'status': 'success',
                'mape': round(mape / 100, 4),  # è½¬æ¢ä¸ºå°æ•°å½¢å¼ (5.25% -> 0.0525)
                'r2_score': round(r2, 3),  # ä½¿ç”¨æ­£ç¡®çš„ key åç§°åŒ¹é…å‰ç«¯
                'sample_count': len(y_true),
                'last_data_point': last_time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            print(f"   âœ… è¯„ä¼°å®Œæˆ: MAPE={mape:.2f}%, R2={metrics['r2_score']}")
            return metrics
            
        except Exception as e:
            print(f"   âŒ åœ¨çº¿è¯„ä¼°å¤±è´¥: {str(e)}")
            return {'status': 'error', 'message': str(e)}
        
