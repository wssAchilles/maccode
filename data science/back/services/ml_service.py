"""
æœºå™¨å­¦ä¹ æœåŠ¡æ¨¡å— - èƒ½æºè´Ÿè½½é¢„æµ‹
ML Service Module for Energy Load Prediction

æ”¯æŒå¤šç§æ¨¡å‹ï¼šRandomForestã€LightGBMã€XGBoost
è‡ªåŠ¨æ¨¡å‹é€‰æ‹©å’Œè¶…å‚æ•°ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import joblib
import os
import tempfile
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings

warnings.filterwarnings('ignore')

# å¯é€‰ä¾èµ–ï¼šLightGBM å’Œ XGBoost
try:
    from lightgbm import LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ RandomForest")

try:
    from xgboost import XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ RandomForest")


from config import Config

class EnergyPredictor:
    """
    èƒ½æºè´Ÿè½½é¢„æµ‹å™¨
    
    ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹æœªæ¥24å°æ—¶çš„èƒ½æºè´Ÿè½½
    æ”¯æŒæ¨¡å‹è®­ç»ƒã€ä¿å­˜ã€åŠ è½½å’Œæ¨ç†
    """
    
    def __init__(self, model_path: str = None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰ï¼Œä¸»è¦ä» Firebase Storage åŠ è½½
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        self.script_dir = Path(__file__).parent  # services ç›®å½•
        self.back_dir = self.script_dir.parent  # back ç›®å½•
        
        # Firebase Storage æ¨¡å‹è·¯å¾„ï¼ˆä¸»è¦å­˜å‚¨ä½ç½®ï¼‰
        self.firebase_model_path = 'models/rf_model.joblib'
        
        # æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„ï¼ˆéƒ¨ç½²åŒ…ä¸­è‡ªå¸¦çš„æ¨¡å‹ï¼Œä»…ç”¨äºé¦–æ¬¡åŠ è½½ï¼‰
        if model_path:
            self.local_model_path = Path(model_path)
        else:
            self.local_model_path = self.back_dir / 'models' / 'rf_model.joblib'
        
        # åˆå§‹åŒ– StorageService
        from services.storage_service import StorageService
        self.storage_service = StorageService()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model: Optional[RandomForestRegressor] = None
        
        # åŸºç¡€ç‰¹å¾åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
        self.base_feature_columns = [
            'Hour', 'DayOfWeek', 'Temperature', 'Price',
            'Lag_1h', 'Lag_24h', 'Lag_168h',
            'Rolling_Mean_6h', 'Rolling_Std_6h', 'Rolling_Mean_24h',
            'Temp_x_Hour', 'Lag24_x_DayOfWeek'
        ]
        
        # å¢å¼ºç‰¹å¾åˆ—è¡¨ï¼ˆæ–°å¢çš„æ—¶é—´ç‰¹å¾ï¼‰
        self.enhanced_feature_columns = [
            # åŸºç¡€æ—¶é—´ç‰¹å¾
            'Month', 'Season', 'IsWeekend', 'IsHoliday', 'DayOfMonth', 'WeekOfYear',
            # å¢å¼ºäº¤äº’ç‰¹å¾
            'Temp_x_Season', 'Lag24_x_IsWeekend', 'Hour_x_IsHoliday',
            # å‘¨æœŸç¼–ç ç‰¹å¾
            'Month_Sin', 'Month_Cos', 'Hour_Sin', 'Hour_Cos'
        ]
        
        # å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ï¼ˆåˆå§‹åŒ–ä¸ºåŸºç¡€ç‰¹å¾ï¼‰
        self.feature_columns = self.base_feature_columns.copy()
        self.target_column = 'Site_Load'
        
        print(f"ğŸ“ Firebase Storage æ¨¡å‹è·¯å¾„: {self.firebase_model_path}")
        print(f"ğŸ“ æœ¬åœ°å…œåº•æ¨¡å‹è·¯å¾„: {self.local_model_path}")
    
    def _load_feature_columns_from_metadata(self):
        """
        ä»æ¨¡å‹å…ƒæ•°æ®ä¸­åŠ è½½ç‰¹å¾åˆ—è¡¨
        ç¡®ä¿é¢„æµ‹æ—¶ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾
        """
        try:
            metadata = self.get_model_metadata()
            if metadata and 'feature_engineering' in metadata:
                fe_info = metadata['feature_engineering']
                if fe_info.get('use_enhanced', False):
                    # æ¨¡å‹ä½¿ç”¨äº†å¢å¼ºç‰¹å¾ï¼Œæ›´æ–°ç‰¹å¾åˆ—è¡¨
                    enhanced_list = fe_info.get('enhanced_features_list', [])
                    if enhanced_list:
                        self.feature_columns = self.base_feature_columns.copy()
                        self.feature_columns.extend(enhanced_list)
                        print(f"   âœ“ å·²ä»å…ƒæ•°æ®æ¢å¤ç‰¹å¾åˆ—è¡¨: {len(self.feature_columns)} ä¸ªç‰¹å¾")
                        print(f"     (åŸºç¡€: {len(self.base_feature_columns)}, å¢å¼º: {len(enhanced_list)})")
                else:
                    # æ¨¡å‹åªä½¿ç”¨åŸºç¡€ç‰¹å¾
                    self.feature_columns = self.base_feature_columns.copy()
                    print(f"   âœ“ æ¨¡å‹ä½¿ç”¨åŸºç¡€ç‰¹å¾: {len(self.feature_columns)} ä¸ª")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°ç‰¹å¾å…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€ç‰¹å¾")
        except Exception as e:
            print(f"   âš ï¸  åŠ è½½ç‰¹å¾å…ƒæ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤åŸºç¡€ç‰¹å¾")
    
    def _get_model_type_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹çš„ç±»å‹åç§°"""
        if self.model is None:
            return 'Unknown'
        
        model_class = type(self.model).__name__
        name_map = {
            'RandomForestRegressor': 'Random Forest Regressor',
            'LGBMRegressor': 'LightGBM Regressor',
            'XGBRegressor': 'XGBoost Regressor'
        }
        return name_map.get(model_class, model_class)
    
    def _create_model(self, model_type: str, n_estimators: int = 100, random_state: int = 42):
        """
        åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('randomforest', 'lightgbm', 'xgboost')
            n_estimators: æ ‘çš„æ•°é‡
            random_state: éšæœºç§å­
            
        Returns:
            (model, hyperparameters) å…ƒç»„
        """
        model_type = model_type.lower()
        
        if model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            model = LGBMRegressor(
                n_estimators=n_estimators,
                learning_rate=0.05,
                max_depth=15,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
                verbose=-1
            )
            params = {
                'n_estimators': n_estimators,
                'learning_rate': 0.05,
                'max_depth': 15,
                'num_leaves': 31
            }
        elif model_type == 'xgboost' and XGBOOST_AVAILABLE:
            model = XGBRegressor(
                n_estimators=n_estimators,
                learning_rate=0.05,
                max_depth=10,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=random_state,
                n_jobs=-1,
                verbosity=0
            )
            params = {
                'n_estimators': n_estimators,
                'learning_rate': 0.05,
                'max_depth': 10
            }
        else:
            # é»˜è®¤ä½¿ç”¨ RandomForest
            model = RandomForestRegressor(
                n_estimators=n_estimators,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=random_state,
                n_jobs=-1,
                verbose=0
            )
            params = {
                'n_estimators': n_estimators,
                'max_depth': 20,
                'min_samples_split': 5,
                'min_samples_leaf': 2
            }
        
        return model, params
    
    def _auto_select_best_model(
        self, 
        X_train, y_train, 
        X_test, y_test, 
        random_state: int = 42,
        use_time_series_cv: bool = True,
        n_splits: int = 5
    ) -> tuple:
        """
        è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆæ”¯æŒæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼‰
        
        æ¯”è¾ƒå¤šç§æ¨¡å‹é…ç½®ï¼Œé€‰æ‹©æµ‹è¯•é›† MAE æœ€ä½çš„
        
        Args:
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_test, y_test: æµ‹è¯•æ•°æ®
            random_state: éšæœºç§å­
            use_time_series_cv: æ˜¯å¦ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆé»˜è®¤ Trueï¼‰
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆé»˜è®¤ 5ï¼‰
            
        Returns:
            (best_model, best_params, selection_info) å…ƒç»„
        """
        candidates = []
        results = {}
        cv_details = {}
        
        # å€™é€‰æ¨¡å‹é…ç½®
        model_configs = [
            ('RandomForest', 'randomforest', 150),
            ('RandomForest_200', 'randomforest', 200),
        ]
        
        # å¦‚æœ LightGBM å¯ç”¨ï¼Œæ·»åŠ åˆ°å€™é€‰
        if LIGHTGBM_AVAILABLE:
            model_configs.extend([
                ('LightGBM', 'lightgbm', 200),
                ('LightGBM_300', 'lightgbm', 300),
            ])
        
        # å¦‚æœ XGBoost å¯ç”¨ï¼Œæ·»åŠ åˆ°å€™é€‰
        if XGBOOST_AVAILABLE:
            model_configs.extend([
                ('XGBoost', 'xgboost', 200),
                ('XGBoost_300', 'xgboost', 300),
            ])
        
        print(f"   è¯„ä¼° {len(model_configs)} ç§æ¨¡å‹é…ç½®...")
        if use_time_series_cv:
            print(f"   ğŸ“Š ä½¿ç”¨ TimeSeriesSplit äº¤å‰éªŒè¯ ({n_splits} æŠ˜)")
        
        baseline_mae = None
        best_mae = float('inf')
        best_model = None
        best_params = None
        best_name = None
        
        # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ç”¨äºæ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        if use_time_series_cv:
            X_full = pd.concat([X_train, X_test], ignore_index=True)
            y_full = pd.concat([pd.Series(y_train), pd.Series(y_test)], ignore_index=True)
            tscv = TimeSeriesSplit(n_splits=n_splits)
        
        for name, model_type, n_estimators in model_configs:
            try:
                print(f"   - è®­ç»ƒ {name}...", end=' ')
                model, params = self._create_model(model_type, n_estimators, random_state)
                
                if use_time_series_cv:
                    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_full):
                        X_cv_train, X_cv_val = X_full.iloc[train_idx], X_full.iloc[val_idx]
                        y_cv_train, y_cv_val = y_full.iloc[train_idx], y_full.iloc[val_idx]
                        
                        model_cv, _ = self._create_model(model_type, n_estimators, random_state)
                        model_cv.fit(X_cv_train, y_cv_train)
                        y_cv_pred = model_cv.predict(X_cv_val)
                        cv_scores.append(mean_absolute_error(y_cv_val, y_cv_pred))
                    
                    # è®¡ç®—äº¤å‰éªŒè¯å¹³å‡åˆ†
                    cv_mae = np.mean(cv_scores)
                    cv_std = np.std(cv_scores)
                    
                    # ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                    test_mae = mean_absolute_error(y_test, y_pred)
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    
                    # ä½¿ç”¨äº¤å‰éªŒè¯ MAE ä½œä¸ºé€‰æ‹©ä¾æ®ï¼ˆæ›´å¯é ï¼‰
                    mae = cv_mae
                    rmse = test_rmse
                    
                    cv_details[name] = {
                        'cv_mae_mean': round(cv_mae, 2),
                        'cv_mae_std': round(cv_std, 2),
                        'cv_scores': [round(s, 2) for s in cv_scores]
                    }
                    
                    print(f"CV_MAE={cv_mae:.2f}Â±{cv_std:.2f} kW, Test_MAE={test_mae:.2f} kW")
                else:
                    # åŸæœ‰çš„ç®€å•è®­ç»ƒ-æµ‹è¯•æ‹†åˆ†
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    print(f"MAE={mae:.2f} kW")
                
                results[name] = {'mae': mae, 'rmse': rmse, 'model_type': model_type}
                candidates.append(name)
                
                # è®°å½•åŸºå‡†ï¼ˆç¬¬ä¸€ä¸ª RandomForestï¼‰
                if baseline_mae is None:
                    baseline_mae = mae
                
                # æ›´æ–°æœ€ä½³
                if mae < best_mae:
                    best_mae = mae
                    best_model = model
                    best_params = params
                    best_name = name
                    
            except Exception as e:
                print(f"å¤±è´¥: {str(e)}")
                continue
        
        # è®¡ç®—ç›¸å¯¹åŸºå‡†çš„æå‡
        improvement = 'N/A'
        if baseline_mae and baseline_mae > 0:
            improvement_pct = (baseline_mae - best_mae) / baseline_mae * 100
            improvement = f"{improvement_pct:.1f}%"
        
        print(f"\n   ğŸ† æœ€ä½³æ¨¡å‹: {best_name} (MAE={best_mae:.2f} kW)")
        if improvement != 'N/A' and improvement != '0.0%':
            print(f"   ğŸ“ˆ ç›¸æ¯”åŸºå‡†æå‡: {improvement}")
        
        selection_info = {
            'winner': best_name,
            'candidates_evaluated': candidates,
            'all_scores': {k: {'mae': round(v['mae'], 2), 'rmse': round(v['rmse'], 2)} 
                          for k, v in results.items()},
            'improvement': improvement,
            'validation_method': 'TimeSeriesSplit' if use_time_series_cv else 'HoldOut',
            'cv_folds': n_splits if use_time_series_cv else None,
            'cv_details': cv_details if use_time_series_cv else None
        }
        
        return best_model, best_params, selection_info
    
    def _get_price(self, hour: int) -> float:
        """
        æ ¹æ®å°æ—¶è¿”å›å³°è°·ç”µä»· (ä»é…ç½®è¯»å–)
        
        Args:
            hour: å°æ—¶ (0-23)
            
        Returns:
            ç”µä»· (å…ƒ/kWh)
        """
        schedule = Config.PRICE_SCHEDULE
        
        if hour in schedule['peak_hours_list']:
            return schedule['peak']
        elif hour in schedule['normal_hours_list']:
            return schedule['normal']
        else:
            return schedule['valley']
    
    def _save_model_metadata(self, metadata: dict) -> bool:
        """
        ä¿å­˜æ¨¡å‹å…ƒæ•°æ®åˆ° Firebase Storage (JSON æ–‡ä»¶)
        
        Args:
            metadata: æ¨¡å‹å…ƒæ•°æ®å­—å…¸
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import json
            from datetime import datetime
            
            # æ·»åŠ æ›´æ–°æ—¶é—´æˆ³
            metadata['updated_at'] = datetime.now().isoformat()
            
            # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            json_data = json.dumps(metadata, indent=2, ensure_ascii=False)
            
            # ä¸Šä¼ åˆ° Storage (ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿ Content-Type æ­£ç¡®)
            import tempfile
            metadata_path = 'models/model_metadata.json'
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
                f.write(json_data)
                temp_path = f.name
            
            try:
                with open(temp_path, 'rb') as f:
                    self.storage_service.bucket.blob(metadata_path).upload_from_file(
                        f, 
                        content_type='application/json'
                    )
            finally:
                import os
                os.unlink(temp_path)
            
            print(f"   âœ“ æ¨¡å‹å…ƒæ•°æ®å·²ä¿å­˜åˆ° Firebase Storage: {metadata_path}")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¿å­˜æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    @staticmethod
    def get_model_metadata() -> Optional[dict]:
        """
        ä» Firebase Storage è·å–æ¨¡å‹å…ƒæ•°æ® (JSON æ–‡ä»¶)
        
        Returns:
            æ¨¡å‹å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        try:
            import json
            from services.storage_service import StorageService
            
            # åˆ›å»º Storage æœåŠ¡å®ä¾‹
            storage = StorageService()
            
            # ä¸‹è½½å…ƒæ•°æ® JSON
            metadata_path = 'models/model_metadata.json'
            json_bytes = storage.download_file(metadata_path)
            
            # è§£æ JSON
            metadata = json.loads(json_bytes.decode('utf-8'))
            return metadata
                
        except Exception as e:
            print(f"è·å–æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def train_model(
        self, 
        data_path: str = None,
        n_estimators: int = 100,
        test_size: float = 0.2,
        random_state: int = 42,
        use_firebase_storage: bool = True,
        auto_select_model: bool = True,
        model_type: str = 'auto',
        use_enhanced_features: bool = True,
        use_time_series_cv: bool = True,
        cv_folds: int = 5
    ) -> Dict[str, float]:
        """
        è®­ç»ƒé¢„æµ‹æ¨¡å‹ï¼ˆæ”¯æŒè‡ªåŠ¨æ¨¡å‹é€‰æ‹©ã€å¢å¼ºç‰¹å¾å’Œæ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼‰
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º data/processed/cleaned_energy_data_all.csv
            n_estimators: æ ‘çš„æ•°é‡ï¼ˆç”¨äºéè‡ªåŠ¨æ¨¡å¼ï¼‰
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            use_firebase_storage: æ˜¯å¦ä» Firebase Storage ä¸‹è½½æ•°æ® (GAE ç¯å¢ƒå¿…é¡»ä¸º True)
            auto_select_model: æ˜¯å¦è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆé»˜è®¤ Trueï¼‰
            model_type: æŒ‡å®šæ¨¡å‹ç±»å‹ ('auto', 'randomforest', 'lightgbm', 'xgboost')
            use_enhanced_features: æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰¹å¾ï¼ˆæœˆä»½ã€å­£èŠ‚ã€èŠ‚å‡æ—¥ç­‰ï¼Œé»˜è®¤ Trueï¼‰
            use_time_series_cv: æ˜¯å¦ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆé»˜è®¤ Trueï¼‰
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆé»˜è®¤ 5ï¼‰
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ (MAE, RMSE, model_type, hyperparameters)
        """
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹è®­ç»ƒèƒ½æºè´Ÿè½½é¢„æµ‹æ¨¡å‹")
        print("="*80 + "\n")
        
        temp_data_path = None
        
        try:
            # ä» Firebase Storage ä¸‹è½½æ•°æ®
            if use_firebase_storage:
                print("ğŸ“¥ ä» Firebase Storage ä¸‹è½½è®­ç»ƒæ•°æ®...")
                from services.storage_service import StorageService
                
                storage_service = StorageService()
                firebase_path = data_path or 'data/processed/cleaned_energy_data_all.csv'
                
                temp_data_path = storage_service.download_to_temp(firebase_path)
                
                if temp_data_path is None:
                    raise FileNotFoundError(f"æ— æ³•ä» Firebase Storage ä¸‹è½½æ•°æ®: {firebase_path}")
                
                data_path = temp_data_path
                print(f"   âœ“ æ•°æ®å·²ä¸‹è½½åˆ°: {data_path}")
            else:
                # æœ¬åœ°æ–‡ä»¶æ¨¡å¼ (å¼€å‘ç¯å¢ƒ)
                if data_path is None:
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
                    possible_paths = [
                        self.back_dir.parent / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
                        self.back_dir / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
                    ]
                    data_path = None
                    for path in possible_paths:
                        if path.exists():
                            data_path = path
                            break
                    if data_path is None:
                        data_path = possible_paths[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤
                else:
                    data_path = Path(data_path)
            
            # è¯»å–æ•°æ®
            print(f"ğŸ“– è¯»å–æ•°æ®: {data_path}")
            try:
                df = pd.read_csv(data_path, parse_dates=['Date'])
                print(f"   âœ“ æ•°æ®è¯»å–æˆåŠŸ: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
            except FileNotFoundError:
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            except Exception as e:
                raise Exception(f"è¯»å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            required_cols = self.feature_columns + [self.target_column]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            
            # å¤„ç†ç¼ºå¤±å€¼
            print(f"\nğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")
            null_counts = df[required_cols].isnull().sum()
            if null_counts.sum() > 0:
                print(f"   âš ï¸  å‘ç°ç¼ºå¤±å€¼:")
                for col, count in null_counts[null_counts > 0].items():
                    print(f"      - {col}: {count} ä¸ª")
                
                # å¯¹äº Temperatureï¼Œä½¿ç”¨å‡å€¼å¡«å……
                if 'Temperature' in null_counts and null_counts['Temperature'] > 0:
                    mean_temp = df['Temperature'].mean()
                    df['Temperature'].fillna(mean_temp, inplace=True)
                    print(f"   âœ“ Temperature ç¼ºå¤±å€¼å·²ç”¨å‡å€¼å¡«å……: {mean_temp:.2f}Â°C")
            else:
                print(f"   âœ“ æ— æ ¸å¿ƒåˆ—ç¼ºå¤±å€¼")
            
            # ================================================================
            # åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼ˆæ£€æµ‹æ•°æ®ä¸­å¯ç”¨çš„å¢å¼ºç‰¹å¾ï¼‰
            # ================================================================
            print(f"\nğŸ”§ æ£€æµ‹å¯ç”¨ç‰¹å¾...")
            
            # é¦–å…ˆä½¿ç”¨åŸºç¡€ç‰¹å¾
            available_features = [col for col in self.base_feature_columns if col in df.columns]
            print(f"   âœ“ åŸºç¡€ç‰¹å¾: {len(available_features)}/{len(self.base_feature_columns)}")
            
            # å¦‚æœå¯ç”¨å¢å¼ºç‰¹å¾ï¼Œæ£€æµ‹å¹¶æ·»åŠ 
            enhanced_features_used = []
            if use_enhanced_features:
                for col in self.enhanced_feature_columns:
                    if col in df.columns:
                        available_features.append(col)
                        enhanced_features_used.append(col)
                
                if enhanced_features_used:
                    print(f"   âœ“ å¢å¼ºç‰¹å¾: {len(enhanced_features_used)} ä¸ª")
                    print(f"     {enhanced_features_used}")
                else:
                    print(f"   âš ï¸  æ•°æ®ä¸­æœªæ‰¾åˆ°å¢å¼ºç‰¹å¾ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")
            
            # æ›´æ–°å®é™…ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
            self.feature_columns = available_features
            print(f"   ğŸ“Š æ€»è®¡ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
                
            # æ¸…é™¤å› ç‰¹å¾å·¥ç¨‹ (Lag/Rolling) äº§ç”Ÿçš„ NaN è¡Œ
            # è¿™äº›è¡Œé€šå¸¸ä½äºæ•°æ®é›†å¤´éƒ¨
            before_drop = len(df)
            df.dropna(inplace=True)
            after_drop = len(df)
            if before_drop != after_drop:
                print(f"   âœ‚ï¸  å·²åˆ é™¤ {before_drop - after_drop} è¡ŒåŒ…å« NaN çš„æ ·æœ¬ (Lag/Rolling start-up)")
            
            # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
            print(f"\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
            X = df[self.feature_columns].copy()
            y = df[self.target_column].copy()
            
            print(f"   - ç‰¹å¾åˆ—: {self.feature_columns}")
            print(f"   - ç›®æ ‡å˜é‡: {self.target_column}")
            print(f"   - æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            print(f"\nâœ‚ï¸  åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒé›†: {int((1-test_size)*100)}%, æµ‹è¯•é›†: {int(test_size*100)}%)...")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            
            print(f"   - è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
            print(f"   - æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
            
            # ================================================================
            # è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒ
            # ================================================================
            if auto_select_model and model_type == 'auto':
                print(f"\nğŸ¤– è‡ªåŠ¨æ¨¡å‹é€‰æ‹©æ¨¡å¼...")
                best_model, best_params, selection_info = self._auto_select_best_model(
                    X_train, y_train, X_test, y_test, random_state,
                    use_time_series_cv=use_time_series_cv,
                    n_splits=cv_folds
                )
                self.model = best_model
                selected_model_type = selection_info['winner']
                hyperparameters = best_params
            else:
                # æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç±»å‹
                print(f"\nğŸŒ² è®­ç»ƒ {model_type} æ¨¡å‹...")
                self.model, hyperparameters = self._create_model(
                    model_type, n_estimators, random_state
                )
                self.model.fit(X_train, y_train)
                selected_model_type = model_type
                selection_info = {'winner': model_type, 'candidates_evaluated': [model_type]}
            
            print(f"   âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ! (ç±»å‹: {selected_model_type})")
            
            # è¯„ä¼°æ¨¡å‹
            print(f"\nğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            
            # è®­ç»ƒé›†é¢„æµ‹
            y_train_pred = self.model.predict(X_train)
            train_mae = mean_absolute_error(y_train, y_train_pred)
            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            
            # æµ‹è¯•é›†é¢„æµ‹
            y_test_pred = self.model.predict(X_test)
            test_mae = mean_absolute_error(y_test, y_test_pred)
            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
            
            # è®¡ç®— RÂ² Score (æµ‹è¯•é›†)
            test_r2 = r2_score(y_test, y_test_pred)
            
            # è®¡ç®— MAPE (æµ‹è¯•é›†) - Mean Absolute Percentage Error
            # é¿å…åˆ†æ¯ä¸º 0
            mask = y_test.values != 0
            if np.sum(mask) > 0:
                test_mape = np.mean(np.abs((y_test.values[mask] - y_test_pred[mask]) / y_test.values[mask])) * 100
            else:
                test_mape = 0.0
            
            print(f"\n   è®­ç»ƒé›†æ€§èƒ½:")
            print(f"      - MAE:  {train_mae:.2f} kW")
            print(f"      - RMSE: {train_rmse:.2f} kW")
            
            print(f"\n   æµ‹è¯•é›†æ€§èƒ½:")
            print(f"      - MAE:  {test_mae:.2f} kW")
            print(f"      - RMSE: {test_rmse:.2f} kW")
            print(f"      - RÂ²:   {test_r2:.4f}")
            print(f"      - MAPE: {test_mape:.2f}%")
            
            # ç‰¹å¾é‡è¦æ€§
            print(f"\nğŸ” ç‰¹å¾é‡è¦æ€§:")
            feature_importance = pd.DataFrame({
                'Feature': self.feature_columns,
                'Importance': self.model.feature_importances_
            }).sort_values('Importance', ascending=False)
            
            for _, row in feature_importance.iterrows():
                print(f"      - {row['Feature']}: {row['Importance']:.4f}")
            
            # ä¿å­˜æ¨¡å‹åˆ° Firebase Storage
            print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° Firebase Storage: {self.firebase_model_path}")
            temp_model_path = None
            try:
                # Step A: åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='wb', suffix='.joblib', delete=False) as tmp_file:
                    temp_model_path = tmp_file.name
                    print(f"   - ä¸´æ—¶æ–‡ä»¶: {temp_model_path}")
                
                # Step B: ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶
                joblib.dump(self.model, temp_model_path)
                print(f"   âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶")
                
                # Step B-2: (æ–°å¢) ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°æŒä¹…åŒ–è·¯å¾„ (ç”¨äºå¼€å‘ç¯å¢ƒè°ƒè¯•)
                try:
                    # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
                    self.local_model_path.parent.mkdir(parents=True, exist_ok=True)
                    joblib.dump(self.model, self.local_model_path)
                    print(f"   âœ“ æ¨¡å‹å·²å¤‡ä»½åˆ°æœ¬åœ°è·¯å¾„: {self.local_model_path}")
                except Exception as local_e:
                    print(f"   âš ï¸  æ— æ³•ä¿å­˜æœ¬åœ°æ¨¡å‹å‰¯æœ¬: {str(local_e)}")
                
                # Step C: ä¸Šä¼ åˆ° Firebase Storage
                with open(temp_model_path, 'rb') as f:
                    self.storage_service.upload_file(
                        file_data=f,
                        destination_path=self.firebase_model_path,
                        content_type='application/octet-stream'
                    )
                print(f"   âœ“ æ¨¡å‹å·²ä¸Šä¼ åˆ° Firebase Storage")
                
            except Exception as e:
                print(f"   âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
                raise
            finally:
                # Step D: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_model_path and os.path.exists(temp_model_path):
                    try:
                        os.remove(temp_model_path)
                        print(f"   ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶")
                    except Exception as e:
                        print(f"   âš ï¸  æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            print("\n" + "="*80)
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
            print("="*80 + "\n")
            
            # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®åˆ° Firebase Storage (å…¨å±€å…ƒæ•°æ®)
            # è·å–æ¨¡å‹ç±»å‹åç§°
            model_type_name = self._get_model_type_name()
            
            try:
                metadata = {
                    'model_type': model_type_name,
                    'model_version': datetime.now().strftime('%Y%m%d_%H%M%S'),
                    'trained_at': datetime.now().isoformat(),
                    'metrics': {
                        'train_mae': float(train_mae),
                        'train_rmse': float(train_rmse),
                        'test_mae': float(test_mae),
                        'test_rmse': float(test_rmse),
                        'r2_score': float(test_r2),  # æ–°å¢: RÂ² Score
                        'mape': float(test_mape / 100),  # æ–°å¢: MAPE (å­˜å‚¨ä¸ºå°æ•°å½¢å¼, 5% -> 0.05)
                    },
                    'training_samples': len(df),
                    'data_source': 'CAISO Real-Time Stream',
                    'feature_importance': feature_importance.to_dict('records'),
                    'model_path': self.firebase_model_path,
                    'status': 'active',
                    # æ–°å¢ï¼šç‰¹å¾å·¥ç¨‹ä¿¡æ¯
                    'feature_engineering': {
                        'total_features': len(self.feature_columns),
                        'base_features': len(self.base_feature_columns),
                        'enhanced_features': len(enhanced_features_used) if use_enhanced_features else 0,
                        'enhanced_features_list': enhanced_features_used if use_enhanced_features else [],
                        'use_enhanced': use_enhanced_features
                    }
                }
                
                # æ·»åŠ è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ä¿¡æ¯
                if auto_select_model and model_type == 'auto':
                    metadata['auto_selection'] = {
                        'enabled': True,
                        'candidates_evaluated': selection_info.get('candidates_evaluated', []),
                        'winner': selection_info.get('winner', 'unknown'),
                        'improvement_over_baseline': selection_info.get('improvement', 'N/A'),
                        'all_scores': selection_info.get('all_scores', {}),
                        # æ–°å¢ï¼šäº¤å‰éªŒè¯ä¿¡æ¯
                        'validation_method': selection_info.get('validation_method', 'HoldOut'),
                        'cv_folds': selection_info.get('cv_folds'),
                        'cv_details': selection_info.get('cv_details')
                    }
                    metadata['hyperparameters'] = hyperparameters
                
                self._save_model_metadata(metadata)
            except Exception as e:
                print(f"   âš ï¸  ä¿å­˜æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {str(e)}")
            
            # è¿”å›è¯„ä¼°æŒ‡æ ‡
            return {
                'train_mae': train_mae,
                'train_rmse': train_rmse,
                'test_mae': test_mae,
                'test_rmse': test_rmse,
                'r2_score': test_r2,  # æ–°å¢
                'mape': test_mape / 100,  # æ–°å¢ (å°æ•°å½¢å¼)
                'feature_importance': feature_importance.to_dict('records'),
                'model_type': model_type_name,
                'hyperparameters': hyperparameters if auto_select_model else {'n_estimators': n_estimators},
                'auto_selection': selection_info if auto_select_model and model_type == 'auto' else None,
                # æ–°å¢ï¼šç‰¹å¾å·¥ç¨‹å’Œäº¤å‰éªŒè¯ä¿¡æ¯
                'feature_engineering': {
                    'total_features': len(self.feature_columns),
                    'enhanced_features_used': enhanced_features_used if use_enhanced_features else []
                },
                'validation': {
                    'method': 'TimeSeriesSplit' if use_time_series_cv else 'HoldOut',
                    'cv_folds': cv_folds if use_time_series_cv else None
                }
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_data_path and os.path.exists(temp_data_path):
                try:
                    os.remove(temp_data_path)
                    print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶")
                except Exception as e:
                    print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def load_model(self) -> bool:
        """
        åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆä¼˜å…ˆä» Firebase Storageï¼‰
        åŒæ—¶åŠ è½½æ¨¡å‹å…ƒæ•°æ®ä»¥æ¢å¤æ­£ç¡®çš„ç‰¹å¾åˆ—è¡¨
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
            
        Raises:
            FileNotFoundError: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨
            Exception: åŠ è½½æ¨¡å‹æ—¶å‡ºé”™
        """
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹...")
        temp_model_path = None
        
        try:
            # Step A: æ£€æŸ¥ Firebase Storage ä¸­æ˜¯å¦å­˜åœ¨æ¨¡å‹
            print(f"   - æ£€æŸ¥ Firebase Storage: {self.firebase_model_path}")
            
            if self.storage_service.file_exists(self.firebase_model_path):
                print(f"   âœ“ Firebase Storage ä¸­å­˜åœ¨æ¨¡å‹")
                
                # Step B: ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
                temp_model_path = self.storage_service.download_to_temp(self.firebase_model_path)
                
                if temp_model_path is None:
                    raise Exception("ä» Firebase Storage ä¸‹è½½æ¨¡å‹å¤±è´¥")
                
                print(f"   âœ“ æ¨¡å‹å·²ä¸‹è½½åˆ°: {temp_model_path}")
                
                # Step C: ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½æ¨¡å‹
                self.model = joblib.load(temp_model_path)
                print(f"   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (æ¥æº: Firebase Storage)")
                
                # Step D: åŠ è½½æ¨¡å‹å…ƒæ•°æ®ä»¥æ¢å¤ç‰¹å¾åˆ—è¡¨
                self._load_feature_columns_from_metadata()
                
                return True
            
            else:
                # Step E: Firebase ä¸­æ²¡æœ‰æ¨¡å‹ï¼Œå°è¯•åŠ è½½æœ¬åœ°å…œåº•æ¨¡å‹
                print(f"   âš ï¸  Firebase Storage ä¸­æ— æ¨¡å‹ï¼Œå°è¯•åŠ è½½æœ¬åœ°å…œåº•æ¨¡å‹")
                
                if not self.local_model_path.exists():
                    raise FileNotFoundError(
                        f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨:\n"
                        f"  - Firebase Storage: {self.firebase_model_path} (ä¸å­˜åœ¨)\n"
                        f"  - æœ¬åœ°è·¯å¾„: {self.local_model_path} (ä¸å­˜åœ¨)\n"
                        f"è¯·å…ˆè°ƒç”¨ train_model() è®­ç»ƒæ¨¡å‹ã€‚"
                    )
                
                self.model = joblib.load(self.local_model_path)
                print(f"   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (æ¥æº: æœ¬åœ°å…œåº•æ–‡ä»¶)")
                
                # åŒæ ·å°è¯•åŠ è½½å…ƒæ•°æ®
                self._load_feature_columns_from_metadata()
                
                return True
        
        except Exception as e:
            raise Exception(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_model_path and os.path.exists(temp_model_path):
                try:
                    os.remove(temp_model_path)
                    print(f"   ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶")
                except Exception as e:
                    print(f"   âš ï¸  æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _load_history_context(self, end_time: datetime, window_size: int = 200) -> pd.DataFrame:
        """
        åŠ è½½ç”¨äºç‰¹å¾æ„å»ºçš„å†å²æ•°æ®ä¸Šä¸‹æ–‡
        
        Args:
            end_time: æˆªæ­¢æ—¶é—´ï¼ˆä¸åŒ…å«ï¼‰
            window_size: éœ€è¦åŠ è½½çš„å†å²çª—å£å¤§å°ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            åŒ…å« Site_Load, Temperature ç­‰åˆ—çš„å†å² DataFrame
        """
        print(f"ğŸ“– åŠ è½½å†å²ä¸Šä¸‹æ–‡ (æˆªæ­¢ {end_time})...")
        
        # å°è¯•åŠ è½½å…¨é‡æ•°æ®æ–‡ä»¶
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™åº”è¯¥ä¼˜åŒ–ä¸ºåªä»æ•°æ®åº“/Storageè¯»å–éƒ¨åˆ†æ•°æ®
        # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿™é‡Œå¤ç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶
        data_path = None
        
        # å°è¯•ä»æœ¬åœ°æˆ–ä¸´æ—¶ç›®å½•æŸ¥æ‰¾
        possible_paths = [
            self.back_dir.parent / 'data' / 'processed' / 'cleaned_energy_data_all.csv', # å¼€å‘ç¯å¢ƒ
            Path('/tmp/cleaned_energy_data_all.csv'), # ä¸´æ—¶ç›®å½•
            self.back_dir / 'data' / 'processed' / 'cleaned_energy_data_all.csv',
        ]
        
        for path in possible_paths:
            if path.exists():
                data_path = path
                break
                
        if data_path is None:
            # å°è¯•ä» Storage ä¸‹è½½
            print("   ğŸ“¥ æœ¬åœ°æœªæ‰¾åˆ°æ•°æ®ï¼Œå°è¯•ä» Firebase Storage ä¸‹è½½...")
            try:
                data_path = self.storage_service.download_to_temp('data/processed/cleaned_energy_data_all.csv')
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•ä¸‹è½½å†å²æ•°æ®: {e}")
                
        if data_path:
            df = pd.read_csv(data_path, parse_dates=['Date'])
            # ç­›é€‰æˆªæ­¢æ—¶é—´ä¹‹å‰çš„æ•°æ®
            history = df[df['Date'] < end_time].tail(window_size).copy()
            if len(history) < 168:
                print(f"   âš ï¸  å†å²æ•°æ®ä¸è¶³ 168 å°æ—¶ (å®é™…: {len(history)}), ç‰¹å¾å¯èƒ½ä¸å‡†ç¡®")
            return history
        else:
            print("   âš ï¸  æ— æ³•åŠ è½½å†å²ä¸Šä¸‹æ–‡ï¼Œå°†ä½¿ç”¨å…¨ 0 å¡«å…… (ä»…ç”¨äºæµ‹è¯•/å†·å¯åŠ¨)")
            # åˆ›å»ºè™šæ‹Ÿå†å²æ•°æ®
            dates = [end_time - timedelta(hours=i) for i in range(window_size, 0, -1)]
            return pd.DataFrame({
                'Date': dates,
                'Site_Load': [0.0] * window_size,
                'Temperature': [25.0] * window_size,
                'Hour': [d.hour for d in dates],
                'DayOfWeek': [d.weekday() for d in dates]
            })

    def predict_next_24h(
        self,
        start_time: Union[str, datetime],
        temp_forecast_list: Optional[List[float]] = None
    ) -> List[Dict[str, Union[datetime, float]]]:
        """
        é¢„æµ‹æœªæ¥24å°æ—¶çš„èƒ½æºè´Ÿè½½ (é€’å½’é¢„æµ‹æ¨¡å¼)
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            temp_forecast_list: æ¸©åº¦é¢„æµ‹åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")
        
        if isinstance(start_time, str):
            start_time = pd.to_datetime(start_time)
        
        # éªŒè¯æ¸©åº¦é¢„æµ‹åˆ—è¡¨é•¿åº¦
        if temp_forecast_list is not None and len(temp_forecast_list) != 24:
            raise ValueError(f"temp_forecast_list é•¿åº¦å¿…é¡»ä¸º 24ï¼Œå½“å‰ä¸º {len(temp_forecast_list)}")
            
        print(f"\nğŸ”® é€’å½’é¢„æµ‹æœªæ¥24å°æ—¶è´Ÿè½½ (ä» {start_time} å¼€å§‹)...")
        
        if temp_forecast_list is None:
            temp_forecast_list = [25.0] * 24
        
        # 1. åŠ è½½å†å²ä¸Šä¸‹æ–‡ (ç”¨äºæ„å»º Lag/Rolling ç‰¹å¾)
        # æˆ‘ä»¬è‡³å°‘éœ€è¦è¿‡å» 168 å°æ—¶çš„æ•°æ®
        history_df = self._load_history_context(start_time, window_size=200)
        
        # è½¬æ¢ä¸º list ä»¥ä¾¿é«˜æ•ˆ append
        # æˆ‘ä»¬ä¸»è¦éœ€è¦ Site_Load åºåˆ—
        history_loads = history_df['Site_Load'].tolist()
        history_temps = history_df['Temperature'].tolist() # å¦‚æœæœ‰ç”¨åˆ°æ¸©åº¦çš„å†å²ç‰¹å¾
        
        predictions = []
        prediction_results = []
        
        # 2. é€’å½’é¢„æµ‹å¾ªç¯
        current_time = start_time
        
        for i in range(24):
            # A. ç‰¹å¾æ„å»º
            hour = current_time.hour
            day_of_week = current_time.weekday()
            temperature = temp_forecast_list[i]
            price = self._get_price(hour)
            
            # æ„å»ºé«˜çº§ç‰¹å¾
            # æ³¨æ„ï¼šhistory_loads çš„æœ€åä¸€ä¸ªå…ƒç´ æ˜¯ t-1 æ—¶åˆ»çš„è´Ÿè½½
            
            # Lag Features
            lag_1h = history_loads[-1] if len(history_loads) >= 1 else 0
            lag_24h = history_loads[-24] if len(history_loads) >= 24 else 0
            lag_168h = history_loads[-168] if len(history_loads) >= 168 else 0
            
            # Rolling Features
            # å–æœ€è¿‘ N ä¸ªç‚¹è®¡ç®—å‡å€¼/æ ‡å‡†å·®
            roll_6h_mean = np.mean(history_loads[-6:]) if len(history_loads) >= 6 else lag_1h
            roll_6h_std = np.std(history_loads[-6:]) if len(history_loads) >= 6 else 0
            roll_24h_mean = np.mean(history_loads[-24:]) if len(history_loads) >= 24 else lag_1h
            
            # Interaction Features (åŸºç¡€)
            temp_x_hour = temperature * hour
            lag24_x_dow = lag_24h * day_of_week
            
            # ç»„è£…åŸºç¡€ç‰¹å¾å‘é‡
            feature_dict = {
                'Hour': hour,
                'DayOfWeek': day_of_week,
                'Temperature': temperature,
                'Price': price,
                'Lag_1h': lag_1h,
                'Lag_24h': lag_24h,
                'Lag_168h': lag_168h,
                'Rolling_Mean_6h': roll_6h_mean,
                'Rolling_Std_6h': roll_6h_std,
                'Rolling_Mean_24h': roll_24h_mean,
                'Temp_x_Hour': temp_x_hour,
                'Lag24_x_DayOfWeek': lag24_x_dow
            }
            
            # æ·»åŠ å¢å¼ºç‰¹å¾ï¼ˆå¦‚æœæ¨¡å‹éœ€è¦ï¼‰
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰¹å¾
            if len(self.feature_columns) > 12:
                # æ—¶é—´ç‰¹å¾
                month = current_time.month
                day_of_month = current_time.day
                week_of_year = current_time.isocalendar()[1]
                
                # å­£èŠ‚ (åŒ—åŠçƒ)
                if month in [3, 4, 5]:
                    season = 0  # æ˜¥
                elif month in [6, 7, 8]:
                    season = 1  # å¤
                elif month in [9, 10, 11]:
                    season = 2  # ç§‹
                else:
                    season = 3  # å†¬
                
                # æ˜¯å¦å‘¨æœ«
                is_weekend = 1 if day_of_week >= 5 else 0
                
                # æ˜¯å¦èŠ‚å‡æ—¥ï¼ˆç¾å›½åŠ å·ï¼‰
                try:
                    import holidays
                    us_ca_holidays = holidays.US(state='CA')
                    is_holiday = 1 if current_time.date() in us_ca_holidays else 0
                except ImportError:
                    is_holiday = is_weekend  # ç®€åŒ–ï¼šå‘¨æœ«è§†ä¸ºå‡æ—¥
                
                # å¢å¼ºäº¤äº’ç‰¹å¾
                temp_x_season = temperature * season
                lag24_x_is_weekend = lag_24h * is_weekend
                hour_x_is_holiday = hour * is_holiday
                
                # å‘¨æœŸç¼–ç 
                month_sin = np.sin(2 * np.pi * month / 12)
                month_cos = np.cos(2 * np.pi * month / 12)
                hour_sin = np.sin(2 * np.pi * hour / 24)
                hour_cos = np.cos(2 * np.pi * hour / 24)
                
                # æ·»åŠ å¢å¼ºç‰¹å¾
                feature_dict.update({
                    'Month': month,
                    'Season': season,
                    'IsWeekend': is_weekend,
                    'IsHoliday': is_holiday,
                    'DayOfMonth': day_of_month,
                    'WeekOfYear': week_of_year,
                    'Temp_x_Season': temp_x_season,
                    'Lag24_x_IsWeekend': lag24_x_is_weekend,
                    'Hour_x_IsHoliday': hour_x_is_holiday,
                    'Month_Sin': month_sin,
                    'Month_Cos': month_cos,
                    'Hour_Sin': hour_sin,
                    'Hour_Cos': hour_cos
                })
            
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸æ¨¡å‹ä¸€è‡´
            features = pd.DataFrame([{col: feature_dict[col] for col in self.feature_columns}])
            
            # B. å•æ­¥æ¨ç†
            pred_load = float(self.model.predict(features)[0])
            
            # C. æ›´æ–°å†å²åºåˆ— (é€’å½’å…³é”®)
            # å°†é¢„æµ‹å€¼ä½œä¸º"çœŸå®å€¼"åŠ å…¥å†å²ï¼Œç”¨äºä¸‹ä¸€æ­¥é¢„æµ‹
            history_loads.append(pred_load)
            predictions.append(pred_load)
            
            # D. è®°å½•ç»“æœ
            prediction_results.append({
                'datetime': current_time,
                'predicted_load': pred_load,
                'temperature': temperature,
                'price': price,
                'hour': hour,
                'day_of_week': day_of_week
            })
            
            # æ—¶é—´æ­¥è¿›
            current_time += timedelta(hours=1)
            
        print(f"   âœ“ é€’å½’é¢„æµ‹å®Œæˆ")
        return prediction_results

    def predict_single(self, *args, **kwargs):
        """
        å•ç‚¹é¢„æµ‹å·²è¢«é€’å½’é¢„æµ‹å–ä»£ï¼Œä¸”ä¸ä»…ä¾èµ–ç®€å•è¾“å…¥ã€‚
        ä¸ºä¿æŒæ¥å£å…¼å®¹æŠ›å‡ºå¼‚å¸¸æˆ–ä»…åšç®€å•å¤„ç†ã€‚
        """
        raise NotImplementedError("å•ç‚¹é¢„æµ‹ (predict_single) å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ predict_next_24h è¿›è¡Œåºåˆ—é¢„æµ‹ã€‚")
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        è·å–æ¨¡å‹çš„å…¨å±€ç‰¹å¾é‡è¦æ€§
        
        ä½¿ç”¨éšæœºæ£®æ—å†…ç½®çš„ feature_importances_ å±æ€§ï¼Œ
        åæ˜ å„ç‰¹å¾åœ¨æ•´ä½“é¢„æµ‹ä¸­çš„å¹³å‡è´¡çŒ®
        
        Returns:
            ç‰¹å¾ååˆ°é‡è¦æ€§åˆ†æ•°çš„æ˜ å°„å­—å…¸
            
        Raises:
            ValueError: æ¨¡å‹æœªåŠ è½½
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")
        
        importance_dict = dict(zip(
            self.feature_columns,
            [float(v) for v in self.model.feature_importances_]
        ))
        
        # æŒ‰é‡è¦æ€§é™åºæ’åº
        sorted_importance = dict(sorted(
            importance_dict.items(),
            key=lambda x: x[1],
            reverse=True
        ))
        
        return sorted_importance
    
    def explain_prediction(
        self,
        hour: int,
        day_of_week: int,
        temperature: float,
        price: float = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ SHAP è§£é‡Šå•æ¬¡é¢„æµ‹
        
        ä¸ºç»™å®šè¾“å…¥æä¾›è¯¦ç»†çš„ç‰¹å¾è´¡çŒ®åˆ†æï¼Œ
        å±•ç¤ºæ¯ä¸ªç‰¹å¾å¦‚ä½•å½±å“æœ€ç»ˆé¢„æµ‹å€¼
        
        Args:
            hour: å°æ—¶ (0-23)
            day_of_week: æ˜ŸæœŸå‡  (0-6)
            temperature: æ¸©åº¦ (Â°C)
            price: ç”µä»·ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ ¹æ®hourè®¡ç®—
            
        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
            - base_value: æ¨¡å‹åŸºå‡†é¢„æµ‹å€¼ï¼ˆè®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ï¼‰
            - predicted_value: å®é™…é¢„æµ‹å€¼
            - feature_contributions: å„ç‰¹å¾çš„è´¡çŒ®å€¼å­—å…¸
            - interpretation: äººç±»å¯è¯»çš„è§£é‡Šæ–‡å­—
        """
        try:
            import shap
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model() æˆ– train_model()")

            # æ„å»ºç‰¹å¾ DataFrame
            # å¦‚æœ price ä¸º Noneï¼Œæ ¹æ® hour è‡ªåŠ¨è®¡ç®—ï¼ˆå³°è°·ç”µä»·ï¼‰
            if price is None:
                # å³°æ—¶æ®µ: 8-22ç‚¹ï¼Œè°·æ—¶æ®µ: 22-8ç‚¹
                if 8 <= hour < 22:
                    price = 1.2  # å³°æ—¶ç”µä»·
                else:
                    price = 0.6  # è°·æ—¶ç”µä»·
            
            # å°è¯•ä»å†å²æ•°æ®è·å–æ»åç‰¹å¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
            # é»˜è®¤å€¼åŸºäºå…¸å‹çš„è´Ÿè½½æ¨¡å¼
            default_load = 150.0  # å…¸å‹å¹³å‡è´Ÿè½½ (kW)
            
            # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾ DataFrameï¼ˆåŒ…å«æ‰€æœ‰12ä¸ªç‰¹å¾ï¼‰
            features = pd.DataFrame({
                'Hour': [hour],
                'DayOfWeek': [day_of_week],
                'Temperature': [temperature],
                'Price': [price],
                'Lag_1h': [default_load],  # 1å°æ—¶å‰çš„è´Ÿè½½
                'Lag_24h': [default_load],  # 24å°æ—¶å‰çš„è´Ÿè½½
                'Lag_168h': [default_load],  # 168å°æ—¶(ä¸€å‘¨)å‰çš„è´Ÿè½½
                'Rolling_Mean_6h': [default_load],  # 6å°æ—¶æ»šåŠ¨å¹³å‡
                'Rolling_Std_6h': [default_load * 0.1],  # 6å°æ—¶æ»šåŠ¨æ ‡å‡†å·®
                'Rolling_Mean_24h': [default_load],  # 24å°æ—¶æ»šåŠ¨å¹³å‡
                'Temp_x_Hour': [temperature * hour],  # æ¸©åº¦ä¸å°æ—¶çš„äº¤äº’ç‰¹å¾
                'Lag24_x_DayOfWeek': [default_load * day_of_week]  # 24å°æ—¶æ»åä¸æ˜ŸæœŸçš„äº¤äº’
            })
            
            # ç¡®ä¿ç‰¹å¾åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
            features = features[self.feature_columns]
            
            # ä½¿ç”¨ TreeExplainer è§£é‡Šéšæœºæ£®æ—æ¨¡å‹
            # åªæœ‰å½“ explainer å°šæœªåˆå§‹åŒ–æ—¶æ‰åˆ›å»ºï¼Œé¿å…é‡å¤è®¡ç®—
            if not hasattr(self, '_shap_explainer') or self._shap_explainer is None:
                self._shap_explainer = shap.TreeExplainer(self.model)
                
            # è®¡ç®— SHAP å€¼
            shap_values = self._shap_explainer.shap_values(features)
            
            # è·å–æœŸæœ›å€¼ (base value)
            # å¯¹äºå›å½’æ¨¡å‹ï¼Œexpected_value åº”è¯¥æ˜¯ä¸€ä¸ªæ ‡é‡
            base_value = self._shap_explainer.expected_value
            if isinstance(base_value, np.ndarray):
                base_value = base_value[0]
                
            # è·å–å½“å‰é¢„æµ‹çš„ SHAP å€¼
            # shap_values å¯¹äºå›å½’å¯èƒ½æ˜¯ (n_samples, n_features)
            # æˆ‘ä»¬åªéœ€è¦ç¬¬ä¸€ä¸ªæ ·æœ¬
            current_shap_values = shap_values[0]
            
            # é¢„æµ‹å€¼ = base_value + sum(shap_values)
            predicted_value = base_value + np.sum(current_shap_values)
            
            # æ„å»ºç‰¹å¾è´¡çŒ®åˆ—è¡¨
            contributions = []
            for i, col in enumerate(self.feature_columns):
                contributions.append({
                    'feature': col,
                    'value': float(features.iloc[0][col]),
                    'contribution': float(current_shap_values[i])
                })
            
            # æŒ‰è´¡çŒ®ç»å¯¹å€¼æ’åº
            contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)
            
            # ç”Ÿæˆäººç±»å¯è¯»çš„è§£é‡Šæ–‡å­—
            top_feature = contributions[0]
            direction = "å¢åŠ " if top_feature['contribution'] > 0 else "å‡å°‘"
            interpretation = (
                f"{top_feature['feature']} æ˜¯å½±å“æœ€å¤§çš„å› ç´ ï¼Œ"
                f"å®ƒä½¿å¾—é¢„æµ‹è´Ÿè½½{direction}äº† {abs(top_feature['contribution']):.1f} kWã€‚"
            )

            return {
                'base_value': float(base_value),
                'predicted_value': float(predicted_value),
                'feature_contributions': contributions,
                'interpretation': interpretation
            }
            
        except Exception as e:
            print(f"è§£é‡Šé¢„æµ‹å¤±è´¥: {str(e)}")
            return None

    def evaluate_recent_performance(self, hours: int = 24) -> Dict[str, Union[float, str]]:
        """
        è¯„ä¼°æ¨¡å‹åœ¨æœ€è¿‘ä¸€æ®µæ—¶é—´çš„è¡¨ç° (åœ¨çº¿ç›‘æ§)
        é€šè¿‡å›æµ‹æœ€è¿‘çš„çœŸå®æ•°æ®æ¥è®¡ç®—æŒ‡æ ‡
        
        Args:
            hours: å›æµ‹çš„å°æ—¶æ•° (é»˜è®¤ 24)
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ (mape, r2, last_update_time)
        """
        print(f"\nğŸ” å¼€å§‹åœ¨çº¿æ¨¡å‹è¯„ä¼° (æœ€è¿‘ {hours} å°æ—¶)...")
        
        try:
            # 1. åŠ¨æ€ä¸‹è½½æœ€æ–°æ•°æ® (ä» Firestore/Storage æŒä¹…åŒ–çš„ CSV)
            from services.storage_service import StorageService
            storage_service = StorageService()
            
            # å°è¯•ä¸‹è½½æœ€æ–°çš„ cleaned_energy_data_all.csv
            data_path = storage_service.download_to_temp('data/processed/cleaned_energy_data_all.csv')
            
            if not data_path:
                print("   âš ï¸ æ— æ³•ä¸‹è½½æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°")
                return {'status': 'no_data'}
                
            # 2. è¯»å–æ•°æ®
            df = pd.read_csv(data_path, parse_dates=['Date'])
            
            # 3. åŸºäºæ—¶é—´æˆªå–æœ€è¿‘ N å°æ—¶çš„æ•°æ® (é¿å…æ•°æ®ä¸­æ–­å¯¼è‡´ tail(N) è·¨åº¦è¿‡å¤§)
            last_time = df['Date'].max()
            start_time = last_time - timedelta(hours=hours)
            
            # ç­›é€‰æ—¶é—´çª—å£å†…çš„æ•°æ®
            recent_df = df[df['Date'] > start_time].copy()
            
            # æ£€æŸ¥æ ·æœ¬é‡
            # ç†è®ºä¸Šåº”è¯¥æœ‰ hours ä¸ªæ ·æœ¬ï¼Œå…è®¸æœ‰ä¸€ç‚¹ç¼ºå¤± (e.g. > 50%)
            min_samples = int(hours * 0.5) 
            if len(recent_df) < min_samples:
                print(f"   âš ï¸ æœ€è¿‘ {hours} å°æ—¶å†…æ•°æ®æ ·æœ¬ä¸è¶³ ({len(recent_df)} < {min_samples})ï¼Œè·³è¿‡è¯„ä¼°")
                return {
                    'status': 'insufficient_data',
                    'message': f'Insufficient data in last {hours}h: found {len(recent_df)} samples'
                }
                
            # 4. å‡†å¤‡ç‰¹å¾å’ŒçœŸå®å€¼
            X_recent = recent_df[self.feature_columns]
            y_true = recent_df[self.target_column].values
            
            # 5. è¿›è¡Œé¢„æµ‹
            y_pred = self.model.predict(X_recent)
            
            # 6. è®¡ç®—æŒ‡æ ‡
            # MAPE: Mean Absolute Percentage Error
            # é¿å…åˆ†æ¯ä¸º 0
            mask = y_true != 0
            if np.sum(mask) == 0:
                print("   âš ï¸ æ‰€æœ‰çœŸå®è´Ÿè½½å‡ä¸º 0ï¼Œæ— æ³•è®¡ç®— MAPE")
                mape = 0.0
            else:
                mape = (np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100)
            
            # R2 Score
            if len(y_true) < 2:
                r2 = 0.0  # æ ·æœ¬å¤ªå°‘
            else:
                r2 = r2_score(y_true, y_pred)
            
            # 7. æ ¼å¼åŒ–ç»“æœ
            # æ³¨æ„ï¼šmape å­˜å‚¨ä¸ºå°æ•°å½¢å¼ (0.05 = 5%)ï¼Œä»¥ä¾¿ä¸å‰ç«¯ percent indicator ç›´æ¥å…¼å®¹
            metrics = {
                'status': 'success',
                'mape': round(mape / 100, 4),  # è½¬æ¢ä¸ºå°æ•°å½¢å¼ (5.25% -> 0.0525)
                'r2_score': round(r2, 3),  # ä½¿ç”¨æ­£ç¡®çš„ key åç§°åŒ¹é…å‰ç«¯
                'sample_count': len(y_true),
                'last_data_point': last_time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            print(f"   âœ… è¯„ä¼°å®Œæˆ: MAPE={mape:.2f}%, R2={metrics['r2_score']}")
            return metrics
            
        except Exception as e:
            print(f"   âŒ åœ¨çº¿è¯„ä¼°å¤±è´¥: {str(e)}")
            return {'status': 'error', 'message': str(e)}
        
def main():
    """
    ä¸»å‡½æ•° - æµ‹è¯•ä»£ç 
    """
    print("\n" + "ğŸ¯ " + "="*76)
    print("èƒ½æºè´Ÿè½½é¢„æµ‹ç³»ç»Ÿ - æµ‹è¯•è„šæœ¬")
    print("="*78 + "\n")
    
    # 1. å®ä¾‹åŒ–é¢„æµ‹å™¨
    print("ã€æ­¥éª¤ 1ã€‘å®ä¾‹åŒ– EnergyPredictor")
    print("-" * 80)
    predictor = EnergyPredictor()
    
    # 2. è®­ç»ƒæ¨¡å‹
    print("\nã€æ­¥éª¤ 2ã€‘è®­ç»ƒæ¨¡å‹")
    print("-" * 80)
    metrics = predictor.train_model(n_estimators=100)
    
    # 3. æµ‹è¯•åŠ è½½æ¨¡å‹
    print("\nã€æ­¥éª¤ 3ã€‘æµ‹è¯•åŠ è½½æ¨¡å‹")
    print("-" * 80)
    predictor_new = EnergyPredictor()
    predictor_new.load_model()
    
    # 4. é¢„æµ‹æœªæ¥24å°æ—¶
    print("\nã€æ­¥éª¤ 4ã€‘é¢„æµ‹æœªæ¥24å°æ—¶è´Ÿè½½")
    print("-" * 80)
    
    # ä½¿ç”¨æ˜å¤©çš„æ—¥æœŸ
    tomorrow = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
    
    # æ¨¡æ‹Ÿæ¸©åº¦é¢„æµ‹ï¼ˆå¤å­£æ¸©åº¦æ¨¡å¼ï¼‰
    temp_forecast = [
        24.0, 23.5, 23.0, 22.8, 22.5, 23.0,  # 00:00-05:00 (å¤œé—´é™æ¸©)
        24.0, 25.0, 26.5, 28.0, 29.5, 30.5,  # 06:00-11:00 (å‡æ¸©)
        31.0, 31.5, 31.8, 31.5, 31.0, 30.0,  # 12:00-17:00 (é«˜æ¸©)
        28.5, 27.0, 26.0, 25.5, 25.0, 24.5   # 18:00-23:00 (é™æ¸©)
    ]
    
    predictions = predictor_new.predict_next_24h(
        start_time=tomorrow,
        temp_forecast_list=temp_forecast
    )
    
    # 5. æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    print("\nã€æ­¥éª¤ 5ã€‘é¢„æµ‹ç»“æœå±•ç¤º")
    print("-" * 80)
    print(f"\né¢„æµ‹æ—¥æœŸ: {tomorrow.strftime('%Y-%m-%d')}\n")
    print(f"{'æ—¶é—´':<12} {'é¢„æµ‹è´Ÿè½½':<12} {'æ¸©åº¦':<10} {'ç”µä»·':<10} {'æ—¶æ®µ':<10}")
    print("-" * 80)
    
    for pred in predictions[:12]:  # åªæ˜¾ç¤ºå‰12å°æ—¶
        dt = pred['datetime']
        load = pred['predicted_load']
        temp = pred['temperature']
        price = pred['price']
        
        # åˆ¤æ–­æ—¶æ®µ
        if price == 0.3:
            period = "è°·æ—¶"
        elif price == 0.6:
            period = "å¹³æ—¶"
        else:
            period = "å³°æ—¶"
        
        print(f"{dt.strftime('%H:%M'):<12} {load:>8.2f} kW  {temp:>6.1f}Â°C  {price:>6.2f}å…ƒ  {period:<10}")
    
    print("... (å12å°æ—¶çœç•¥)")
    
    # 6. å•ç‚¹é¢„æµ‹æµ‹è¯•
    print("\nã€æ­¥éª¤ 6ã€‘å•ç‚¹é¢„æµ‹æµ‹è¯•")
    print("-" * 80)
    
    test_cases = [
        (0, 1, 24.0, "å‘¨ä¸€å‡Œæ™¨0ç‚¹ï¼Œ24Â°C"),
        (12, 1, 30.0, "å‘¨ä¸€ä¸­åˆ12ç‚¹ï¼Œ30Â°C"),
        (20, 1, 28.0, "å‘¨ä¸€æ™šä¸Š8ç‚¹ï¼Œ28Â°C"),
    ]
    
    for hour, dow, temp, desc in test_cases:
        pred = predictor_new.predict_single(hour, dow, temp)
        print(f"   {desc}: {pred:.2f} kW")
    
    # 7. æ€»ç»“
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("="*80)
    print(f"\næ¨¡å‹æ€§èƒ½:")
    print(f"   - æµ‹è¯•é›† MAE:  {metrics['test_mae']:.2f} kW")
    print(f"   - æµ‹è¯•é›† RMSE: {metrics['test_rmse']:.2f} kW")
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {predictor.local_model_path}")
    print(f"å¯ä»¥é€šè¿‡ load_model() åŠ è½½ä½¿ç”¨\n")


if __name__ == "__main__":
    main()
