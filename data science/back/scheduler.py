"""
å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
Scheduler for Periodic Tasks

ä½¿ç”¨ APScheduler å®ç°:
1. æ¯å°æ—¶æŠ“å– CAISO å’Œå¤©æ°”æ•°æ®
2. æ¯å¤©å‡Œæ™¨é‡è®­æ¨¡å‹
"""

import os
import logging
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from datetime import datetime
from services.external_data_service import ExternalDataService
from services.ml_service import EnergyPredictor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class DataPipelineScheduler:
    """æ•°æ®ç®¡é“è°ƒåº¦å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è°ƒåº¦å™¨"""
        self.scheduler = BackgroundScheduler(
            timezone='UTC',  # ä½¿ç”¨ UTC æ—¶åŒº
            job_defaults={
                'coalesce': True,  # åˆå¹¶é”™è¿‡çš„ä»»åŠ¡
                'max_instances': 1,  # æ¯ä¸ªä»»åŠ¡æœ€å¤šåŒæ—¶è¿è¡Œ 1 ä¸ªå®ä¾‹
                'misfire_grace_time': 300  # é”™è¿‡ä»»åŠ¡çš„å®½é™æ—¶é—´ (ç§’)
            }
        )
        
        self.external_data_service = ExternalDataService()
        self.energy_predictor = EnergyPredictor()
        
        logger.info("âœ“ DataPipelineScheduler åˆå§‹åŒ–å®Œæˆ")
    
    def fetch_data_job(self):
        """
        æ•°æ®æŠ“å–ä»»åŠ¡ (æ¯å°æ—¶æ‰§è¡Œ)
        
        æ‰§è¡Œå†…å®¹:
        - è·å– CAISO ç”µåŠ›è´Ÿè½½
        - è·å– OpenWeather å¤©æ°”æ•°æ®
        - è¿½åŠ åˆ° Firebase Storage CSV
        """
        logger.info("="*80)
        logger.info("â° å¼€å§‹æ‰§è¡Œæ•°æ®æŠ“å–ä»»åŠ¡")
        logger.info("="*80)
        
        try:
            success = self.external_data_service.fetch_and_publish()
            
            if success:
                logger.info("âœ… æ•°æ®æŠ“å–ä»»åŠ¡å®Œæˆ")
            else:
                logger.error("âŒ æ•°æ®æŠ“å–ä»»åŠ¡å¤±è´¥")
        
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æŠ“å–ä»»åŠ¡å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
        
        logger.info("="*80 + "\n")
    
    def train_model_job(self):
        """
        æ¨¡å‹è®­ç»ƒä»»åŠ¡ (æ¯å¤©æ‰§è¡Œ)
        
        æ‰§è¡Œå†…å®¹:
        - ä» Firebase Storage ä¸‹è½½æœ€æ–°æ•°æ®
        - é‡æ–°è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        - ä¿å­˜æ¨¡å‹åˆ°éƒ¨ç½²åŒ…
        """
        logger.info("="*80)
        logger.info("â° å¼€å§‹æ‰§è¡Œæ¨¡å‹è®­ç»ƒä»»åŠ¡")
        logger.info("="*80)
        
        try:
            # ä½¿ç”¨ Firebase Storage æ•°æ®è®­ç»ƒ
            metrics = self.energy_predictor.train_model(
                use_firebase_storage=True,
                n_estimators=100
            )
            
            logger.info("âœ… æ¨¡å‹è®­ç»ƒä»»åŠ¡å®Œæˆ")
            logger.info(f"   - æµ‹è¯•é›† MAE: {metrics['test_mae']:.2f} kW")
            logger.info(f"   - æµ‹è¯•é›† RMSE: {metrics['test_rmse']:.2f} kW")
        
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒä»»åŠ¡å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
        
        logger.info("="*80 + "\n")
    
    def start(self):
        """
        å¯åŠ¨è°ƒåº¦å™¨
        
        é…ç½®ä»»åŠ¡:
        - æ•°æ®æŠ“å–: æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ (æ•´ç‚¹)
        - æ¨¡å‹è®­ç»ƒ: æ¯å¤©å‡Œæ™¨ 4:00 UTC æ‰§è¡Œ
        """
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ å¯åŠ¨æ•°æ®ç®¡é“è°ƒåº¦å™¨")
        logger.info("="*80)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ GAE ç¯å¢ƒä¸­
        is_gae = os.getenv('GAE_ENV', '').startswith('standard')
        
        if is_gae:
            logger.info("ğŸ“ è¿è¡Œç¯å¢ƒ: Google App Engine")
        else:
            logger.info("ğŸ“ è¿è¡Œç¯å¢ƒ: æœ¬åœ°å¼€å‘")
        
        # 1. æ•°æ®æŠ“å–ä»»åŠ¡ (æ¯å°æ—¶æ•´ç‚¹æ‰§è¡Œ)
        self.scheduler.add_job(
            func=self.fetch_data_job,
            trigger=CronTrigger(minute=0, timezone='UTC'),  # æ¯å°æ—¶çš„ç¬¬ 0 åˆ†é’Ÿ
            id='fetch_data_hourly',
            name='æ•°æ®æŠ“å–ä»»åŠ¡ (æ¯å°æ—¶)',
            replace_existing=True
        )
        logger.info("âœ“ å·²æ·»åŠ ä»»åŠ¡: æ•°æ®æŠ“å– (æ¯å°æ—¶æ•´ç‚¹)")
        
        # 2. æ¨¡å‹è®­ç»ƒä»»åŠ¡ (æ¯å¤©å‡Œæ™¨ 4:00 UTC æ‰§è¡Œ)
        self.scheduler.add_job(
            func=self.train_model_job,
            trigger=CronTrigger(hour=4, minute=0, timezone='UTC'),
            id='train_model_daily',
            name='æ¨¡å‹è®­ç»ƒä»»åŠ¡ (æ¯å¤©å‡Œæ™¨ 4:00 UTC)',
            replace_existing=True
        )
        logger.info("âœ“ å·²æ·»åŠ ä»»åŠ¡: æ¨¡å‹è®­ç»ƒ (æ¯å¤©å‡Œæ™¨ 4:00 UTC)")
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start()
        logger.info("âœ… è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        # æ‰“å°ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
        jobs = self.scheduler.get_jobs()
        logger.info(f"\nğŸ“‹ å·²æ³¨å†Œä»»åŠ¡ ({len(jobs)} ä¸ª):")
        for job in jobs:
            next_run = job.next_run_time
            if next_run:
                logger.info(f"   - {job.name}")
                logger.info(f"     ä¸‹æ¬¡æ‰§è¡Œ: {next_run.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        
        logger.info("="*80 + "\n")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        logger.info("ğŸ›‘ åœæ­¢è°ƒåº¦å™¨...")
        self.scheduler.shutdown(wait=False)
        logger.info("âœ“ è°ƒåº¦å™¨å·²åœæ­¢")
    
    def run_now(self, job_name='fetch_data'):
        """
        ç«‹å³æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ (ç”¨äºæµ‹è¯•)
        
        Args:
            job_name: ä»»åŠ¡åç§° ('fetch_data' æˆ– 'train_model')
        """
        logger.info(f"\nğŸ§ª æ‰‹åŠ¨è§¦å‘ä»»åŠ¡: {job_name}")
        
        if job_name == 'fetch_data':
            self.fetch_data_job()
        elif job_name == 'train_model':
            self.train_model_job()
        else:
            logger.error(f"âŒ æœªçŸ¥ä»»åŠ¡: {job_name}")


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_scheduler_instance = None


def get_scheduler():
    """
    è·å–è°ƒåº¦å™¨å•ä¾‹
    
    Returns:
        DataPipelineScheduler: è°ƒåº¦å™¨å®ä¾‹
    """
    global _scheduler_instance
    
    if _scheduler_instance is None:
        _scheduler_instance = DataPipelineScheduler()
    
    return _scheduler_instance


def init_scheduler():
    """
    åˆå§‹åŒ–å¹¶å¯åŠ¨è°ƒåº¦å™¨
    
    æ­¤å‡½æ•°åº”åœ¨ Flask åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
    æ³¨æ„: åœ¨ GAE å¤šå®ä¾‹ç¯å¢ƒä¸­ï¼Œæ¯ä¸ªå®ä¾‹éƒ½ä¼šè¿è¡Œè°ƒåº¦å™¨
    """
    # é˜²æ­¢åœ¨ Flask é‡è½½æ—¶é‡å¤å¯åŠ¨
    # é˜²æ­¢åœ¨ Flask é‡è½½æ—¶é‡å¤å¯åŠ¨
    # WERKZEUG_RUN_MAIN ä¸º true è¡¨ç¤ºè¿™æ˜¯ Werkzeug é‡è½½å™¨ç”Ÿæˆçš„å­è¿›ç¨‹ï¼ˆå®é™…è¿è¡ŒæœåŠ¡çš„è¿›ç¨‹ï¼‰
    # åœ¨ GAE ç¯å¢ƒä¸­ï¼Œæ²¡æœ‰ Werkzeugé‡è½½å™¨ï¼Œæ‰€ä»¥ç›´æ¥æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»ç¨‹åº
    if os.getenv('GAE_ENV', '').startswith('standard') or os.getenv('WERKZEUG_RUN_MAIN') == 'true':
        scheduler = get_scheduler()
        if not scheduler.scheduler.running:
            scheduler.start()
        return scheduler
    else:
        # æœ¬åœ°å¼€å‘çš„ä¸»è¿›ç¨‹ï¼ˆç›‘æ§è¿›ç¨‹ï¼‰ï¼Œä¸å¯åŠ¨è°ƒåº¦å™¨ï¼Œé˜²æ­¢åŒé‡å¯åŠ¨
        logger.info("â¸ï¸  è·³è¿‡è°ƒåº¦å™¨å¯åŠ¨ (ä¸è®ºæ˜¯ Flask ç›‘æ§è¿›ç¨‹è¿˜æ˜¯éè¿è¡ŒçŠ¶æ€)")
        return None


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("\nğŸ§ª æµ‹è¯•è°ƒåº¦å™¨\n")
    
    scheduler = DataPipelineScheduler()
    
    # æµ‹è¯•æ•°æ®æŠ“å–
    print("ã€æµ‹è¯• 1ã€‘ç«‹å³æ‰§è¡Œæ•°æ®æŠ“å–ä»»åŠ¡")
    scheduler.run_now('fetch_data')
    
    # å¯é€‰: æµ‹è¯•æ¨¡å‹è®­ç»ƒ (éœ€è¦è¾ƒé•¿æ—¶é—´)
    # print("\nã€æµ‹è¯• 2ã€‘ç«‹å³æ‰§è¡Œæ¨¡å‹è®­ç»ƒä»»åŠ¡")
    # scheduler.run_now('train_model')
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
