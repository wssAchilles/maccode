{
  "components": {
    "comp-export-data-op": {
      "executorLabel": "exec-export-data-op",
      "inputDefinitions": {
        "parameters": {
          "bq_table_id": {
            "parameterType": "STRING"
          },
          "bucket_name": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-trigger-tuning-job-op": {
      "executorLabel": "exec-trigger-tuning-job-op",
      "inputDefinitions": {
        "parameters": {
          "epochs": {
            "defaultValue": 5.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "location": {
            "parameterType": "STRING"
          },
          "model_display_name": {
            "parameterType": "STRING"
          },
          "project": {
            "parameterType": "STRING"
          },
          "training_data_uri": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://sentinel-mlops-artifacts-sentinel-ai-project-482208/pipeline_root",
  "deploymentSpec": {
    "executors": {
      "exec-export-data-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "export_data_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' 'google-cloud-storage' 'pandas' 'pyarrow'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef export_data_op(\n    bq_table_id: str,\n    bucket_name: str,\n) -> str:\n    \"\"\"\n    Exports data from BigQuery, converts to Gemini JSONL format, and uploads to GCS.\n    Returns the GCS URI of the training data.\n    \"\"\"\n    import json\n    import pandas as pd\n    from google.cloud import bigquery\n    from google.cloud import storage\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Initialize clients\n    bq_client = bigquery.Client()\n    storage_client = storage.Client()\n\n    logger.info(f\"Reading data from BigQuery table: {bq_table_id}\")\n\n    # Read from BigQuery\n    # Assuming standard columns for tuning, or fetching raw data to format\n    # For this example, we select all columns and expect 'user_input' and 'model_output' or similar\n    # Adjust SQL based on actual schema. Based on requirements, converting to messages format.\n    query = f\"SELECT * FROM `{bq_table_id}`\"\n    df = bq_client.query(query).to_dataframe()\n\n    if df.empty:\n        raise ValueError(\"BigQuery table is empty, cannot proceed with training.\")\n\n    logger.info(f\"Fetched {len(df)} rows. Converting to JSONL format...\")\n\n    # Convert to JSONL format: {\"messages\": [{\"role\": \"user\", \"content\": ...}, {\"role\": \"model\", \"content\": ...}]}\n    # Assuming dataframe has columns like 'prompt'/'input' and 'response'/'output'\n    # Since specific schema wasn't provided, we'll try to detect or assume standard naming, \n    # but based on prompt \"convert to... format\", we'll assume we need to map rows.\n    # Let's handle a common case or generic structure.\n    # If the table is already in the right format, this step is simpler. \n    # Let's assume the table has 'input_text' and 'output_text' for this implementation.\n\n    jsonl_data = []\n\n    # Robust column detection\n    cols = df.columns.str.lower()\n    input_col = next((c for c in cols if c in ['input', 'prompt', 'user_input', 'question']), None)\n    output_col = next((c for c in cols if c in ['output', 'response', 'model_output', 'answer']), None)\n\n    if not input_col or not output_col:\n        # Fallback: Dump row as string if specific columns not found (Not ideal for tuning)\n        # Or better: raise error if critical columns missing\n        # For safety in this prompt, let's assume 'question' and 'answer' based on typical tuning datasets\n        # Or checking table name 'tuning_dataset' implies readiness. \n        # Let's assume column names based on standard patterns.\n        # However, for now, we will construct based on row content if specific keys exist.\n        pass\n\n    # Actually, let's look at row iteration.\n    for _, row in df.iterrows():\n        # Adjust these column names to match your actual BigQuery table schema\n        user_content = row.get('input_text') or row.get('prompt') or row.get('question')\n        model_content = row.get('output_text') or row.get('response') or row.get('answer')\n\n        if user_content and model_content:\n            entry = {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": str(user_content)},\n                    {\"role\": \"model\", \"content\": str(model_content)}\n                ]\n            }\n            jsonl_data.append(entry)\n\n    if not jsonl_data:\n        raise ValueError(\"Could not extract valid training pairs (input/output) from dataframe.\")\n\n    # Upload to GCS\n    file_name = \"training_data.jsonl\"\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(file_name)\n\n    logger.info(f\"Uploading {len(jsonl_data)} records to gs://{bucket_name}/{file_name}\")\n\n    # Create valid JSONL string\n    jsonl_string = \"\\n\".join([json.dumps(record) for record in jsonl_data])\n    blob.upload_from_string(jsonl_string, content_type=\"application/jsonl\")\n\n    gcs_uri = f\"gs://{bucket_name}/{file_name}\"\n    return gcs_uri\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-trigger-tuning-job-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "trigger_tuning_job_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef trigger_tuning_job_op(\n    training_data_uri: str,\n    project: str,\n    location: str,\n    model_display_name: str,\n    epochs: int = 5,\n) -> str:\n    \"\"\"\n    Triggers a Supervised Fine-Tuning job for Gemini using Vertex AI SDK.\n    \"\"\"\n    import vertexai\n    from vertexai.preview.tuning import sft\n    import logging\n    import time\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    logger.info(f\"Initializing Vertex AI for project {project} in {location}\")\n    vertexai.init(project=project, location=location)\n\n    source_model = \"gemini-1.5-pro-002\"\n\n    logger.info(f\"Submitting tuning job for {source_model} with data {training_data_uri}\")\n\n    # sft.train returns a TuningJob object\n    sft_tuning_job = sft.train(\n        source_model=source_model,\n        train_dataset=training_data_uri,\n        # The prompt implies we return a resource name. sft.train is synchronous-like or returns job?\n        # Typically SDK methods start the job.\n        epochs=epochs,\n        tuned_model_display_name=model_display_name,\n    )\n\n    # Wait for job submission to be confirmed (status check usually happens internally or we can just return ID)\n    # The requirement is 'Return Job Resource Name'.\n\n    # Check if sft_tuning_job has 'resource_name' attribute immediately \n    # or if we need to access underlying job details.\n    # Note: sft.train method in preview might block or return a job object. \n    # Usually in Pipelines we don't want to block for hours. \n    # However, standard component execution has a timeout. \n    # If sft.train blocks, this component will run for the duration of training.\n    # If current SDK implementation allows blocking=False, that's preferred.\n    # Assuming default behavior or non-blocking return if possible, or we accept blocking component.\n\n    logger.info(f\"Job state: {sft_tuning_job.state}\")\n    resource_name = sft_tuning_job.resource_name\n    logger.info(f\"Tuning Job Resource Name: {resource_name}\")\n\n    return resource_name\n\n"
          ],
          "image": "python:3.9"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "Pipeline to export data from BigQuery and fine-tune Gemini model",
    "name": "sentinel-continuous-training-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "export-data-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-export-data-op"
          },
          "inputs": {
            "parameters": {
              "bq_table_id": {
                "componentInputParameter": "bq_table_id"
              },
              "bucket_name": {
                "componentInputParameter": "bucket_name"
              }
            }
          },
          "taskInfo": {
            "name": "export-data-op"
          }
        },
        "trigger-tuning-job-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-trigger-tuning-job-op"
          },
          "dependentTasks": [
            "export-data-op"
          ],
          "inputs": {
            "parameters": {
              "epochs": {
                "runtimeValue": {
                  "constant": 5.0
                }
              },
              "location": {
                "componentInputParameter": "location"
              },
              "model_display_name": {
                "componentInputParameter": "model_display_name"
              },
              "project": {
                "componentInputParameter": "project_id"
              },
              "training_data_uri": {
                "taskOutputParameter": {
                  "outputParameterKey": "Output",
                  "producerTask": "export-data-op"
                }
              }
            }
          },
          "taskInfo": {
            "name": "trigger-tuning-job-op"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "bq_table_id": {
          "defaultValue": "sentinel-ai-project-482208.retail_ai.gemini_tuning_dataset",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "bucket_name": {
          "defaultValue": "sentinel-mlops-artifacts-sentinel-ai-project-482208",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "location": {
          "defaultValue": "us-central1",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "model_display_name": {
          "defaultValue": "sentinel-gemini-tuned-v1",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "project_id": {
          "defaultValue": "sentinel-ai-project-482208",
          "isOptional": true,
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.15.2"
}